---
title: "Residual Diagnostics and Forecast Accuracy"
author: "Ankit Tyagi"
date: "12/29/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#########################################################################################################################################################################Residual diagnostic############################################
####################################################################################################################

Fitted Values :- 
1. Each observation in a time series can be forecast using all previous observations. These values are called Fitted Values. 
2. Fitted Values are often not the actual forecasts because any parameters involved in the forecasting method are estimated using all the available observation in the time series, including future observations. For e.g. Average Method and Drift Method. But in case of naive and seasonal naive, the forecast do not involve any parameter so the fitted values are actual forecasts.

Residual :- Residual in a time series model is what is left over after fitting a model. 
$$e_t = y_t - fitted(y_t)$$
If a model fit the data accurately then,

1. The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
2.The residual has zero mean. If the residuals have a mean other than zero, then the forecasts are biased. However, adjusting for bias is easy: if the residuals have mean m, then simply add m to all forecasts and the bias problem is solved.

But there is a possobility that more than one forecasting methods satisfy above mentioned properties. So these properties can not be used to select a forecasting method.

Another 2 useful (not necessary) properties to check are 
1. The residuals have a constant variance.
2. The residuals are normally distributed.

The above 2 properties makes the calculation for prediction interval more easy otherwise they do not cause any problem in the forecasting.

Now we will see an example of residual analysis.
```{r}
library(fpp2)
autoplot(goog200) +
  xlab("Day") + ylab("closing prices(in $") +
  ggtitle("Google Stock(daily ending 6 December 2013")
```

For stock prices, naive method is often the best method.

```{r}
autoplot(residuals(naive(goog200))) + 
  xlab("Day") + ylab("") +
  ggtitle("Residual from the Naive Method")
```

The graph above shows that mean of the residuals is close to zero. Also, the variation of the residuals stays same across the historical data, apart from one outlier.
```{r}
gghistogram(residuals(naive(goog200))) + ggtitle("Histogram of the residuals")
```

The histogram shows that the residuals are not normally distributed. There are some extreme observations on the right (positively skewed). The prediction interval calculated assuming the normal distribution may be inaccurate.
```{r}
ggAcf(residuals(naive(goog200))) + ggtitle("ACF of residuals")
```

The autocorrelation is not significant for any lag. 

From above graph it appears that naive method forecasts accounts for all available information.

Portmanteau tests for autocorrelation :-
One issue with ACF plots is that we make an hypothesis for each of the lag and each test has some probability of giving a false result. If the value of h is large, then it is likely that atleast one of them gives wrong result.
To overcome this we are going to test whether the first h autocorrelations are significantly different from what is expected from a white noise process. 

Box - Pierce test :- 
$$ Q = T \sum_{k=1}^{h} r_{k}^{2}\  ,$$
where h = maximum lag being considered.(usually h = 10 for non seasonal data, h = 2m for seasonal data with seasonal frequency m). The test is still not good if h is too large. So if the values are larger than T/5 use h = T/5.

T = no. of observations.

Ljung-Box test :- $$Q = T(T+2)  \sum_{k=1}^{h}(T-k)^{-1} r_{k}^{2}$$
If the autocorrelation comes with white noise then both Q follows chi square distribution with (h-K) degrees of freedom where K is the number of parameters in the model.

```{r}
Box.test(residuals(naive(goog200)), lag = 10, fitdf = 0)
```
```{r}
Box.test(residuals(naive(goog200)), lag = 10, fitdf = 0, type = "Lj")
```

p values are quite high that means that residuals are not distinguishable from a white noise process.

All of the above analysis can be done using just one function.
```{r}
checkresiduals(naive(goog200))
```

Forecast accuracy

The size of the residuals is not a reliable indication of how large true forecast errors are likely to be. The accuracy of forecasts can only be determined by considering how well a model performs on new data that were not used when fitting the model.

We divide the dataset in training data (80% usually) and test set (20% data). 
Forecast error = $Actual(y_{T+h}) - forc(y_{T+h})$.
Forecast error are different from residuals as the residuals are calculated on the training dataset.

Mean absolute error : MAE :- mean(|$e_t$|).
Root squared mean error : RMSE :- $\sqrt{mean(e_t^2)}$.

A method that minimize MAE leads to forecast of the median and a method that minimise RMSE leads to the forecast of mean.

The issue with MAE and RMSE is that the results are on same scale as of the data. So we cannot compare the accurancy of methods that are based on datasets of different units.

Mean absolute percentage error : MAPE :- mean(|$p_t$|).
where $p_t = 100e_t/y_t$.

Disadvantge of using percentage error :- 
1. Percentages are infinite or undefined for $y_t$ = 0 and having extreme values for $y_t$ close to zero.
2. Percentage errors make sense only when the measurement is on interval scale not on the ratio scale because percentage errors assume that the measurement scale has a meaningful zero.
3. Percentage error ususally put high penalty on the negative error than on positive error as the we always more possibility if making a positive error than making a negetive error.

To solve these issues we might use symmetric MAPE(sMAPE)
sMAPE :- mean( 200 * $|y_t - forc(y_t)|/(y_t + forc(y_t))$ )
This still do not solve the issue mentioned in (1) above.

Scaled Error :- To compare the forecast from different time series, we can use scaled error. For the non-seasonal data scaled error can be given as :-
$$q_j = \frac{e_j}{\frac{1}{T-1} \sum_{t=2}^{T} |y_t - forc(y_t)|}$$
The denominator is measuring the error from a naive forecast method. 
1. $q_j$ is independent of units. 
2. $q_j$ is less than 1 if the forecast is better than the average naive forecast and greater than 1 is the forecast is worse than average naive forecast.

For the seasonal data scaled error can be given as :-
$$q_j = \frac{e_j}{\frac{1}{T-m} \sum_{t=m+1}^{T} |y_t - forc(y_{t-m})|}$$
Mean absolute scaled error (MASE) = mean(|$q_t$|).

We will see an example of this on Quarterly Beer Production data. 
```{r}
beer2 = window(ausbeer, start = 1992, end = c(2007,4))
beerfit1 = meanf(beer2, h = 10)
beerfit2 = rwf(beer2, h = 10)
beerfit3 = snaive(beer2, h = 10)
autoplot(window(ausbeer, start = 1992)) +
  autolayer(beerfit1, series = "Mean", PI = FALSE) +
  autolayer(beerfit2, series = "Naive", PI = FALSE) +
  autolayer(beerfit3, series = "Seasonal Naive", PI = FALSE) +
  ggtitle("Quarterly beer production data") +
  xlab("Years") + ylab("Megalitres") +
  guides(colour = guide_legend(title = "Forecast"))
```

The above graph shows that the seasonal naive fits the data more accurately. We will prove the same using error measures.

```{r}
beer3 = window(ausbeer, start = 2008)
accuracy(beerfit1, beer3)
accuracy(beerfit2, beer3)
accuracy(beerfit3, beer3)
```

The seasonal naive method shows the least test error in all the error measures.
Some times different erroe measures might leads to different conclusion about which is the best forecasting method.

Now we will see a non seasonal data example. Google stock price index
```{r}
googfc1 = meanf(goog200, h = 40)
googfc2 = rwf(goog200, h = 40)
googfc3 = rwf(goog200, drift = TRUE, h = 40)
autoplot(subset(goog, end = 240)) +
  autolayer(googfc1, series = "Mean", PI = FALSE) +
  autolayer(googfc2, series = "Naive", PI = FALSE) +
  autolayer(googfc3, series = "Drift", PI = FALSE) +
  ggtitle("Google Daily Stock Price Index") +
  xlab("Day") + ylab("Closing Price(in $)") +
  guides(colour = guide_legend(title = "Forecast"))
```

The above graph shows that the drift method is giving the best results.
```{r}
googtest = window(goog, start = 201, end = 240)
accuracy(googfc1, googtest)
accuracy(googfc2, googtest)
accuracy(googfc3, googtest)
```

All the error measures shows that drift method gives the best forecast.

Time Series Cross Validation :- In this method we do a series of tests each test contains one test observations and the preceeding that observation is the training set. This way we are not using any future observation for forecast.
Initial observations are not used for cross validation as the forecast on a small dataset is not reliable.

```{r}
e = tsCV(goog200, rwf, drift = TRUE, h = 1)
sqrt(mean(e^2, na.rm = TRUE))
```

6.233245 is our average cross validation error.
```{r}
sqrt(mean(residuals(rwf(goog200, drift = TRUE))^2, na.rm  = TRUE))
```

RMSE from the residuals is obviously smaller as we are using all the available observations including future observations to forecast.

We can also do a multistep forecast cross validation.
```{r}
e = tsCV(goog200, forecastfunction = naive, h = 8)

mse = colMeans(e^2, na.rm = TRUE)

data.frame(h = 1:8, MSE = mse) %>% 
  ggplot(aes(x = h, y = MSE)) + geom_point()
```

The above shows how the forecast error is increasing as the forecast horizon is increasing.
