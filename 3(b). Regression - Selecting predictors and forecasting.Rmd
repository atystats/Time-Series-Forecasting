---
title: "3(b). Regression- selecting predictors and forecasting"
author: "Ankit Tyagi"
date: "1/17/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Fitting Trend :-**
Trend is common among time series. we usually visualize the data and get the idea of the trend. A linear trend can be fitted using the equation :- $$y_t = b_0 + b_1 t + e_t$$
where t = 1,2,....T. 

We can identify the non linear trend and fit a non linear equation as well.

**Fitting Seasonality :-** Suppose that we are forecasting daily data and we want to account for the day of the week as a predictor. Then we create a dummy variable for each day except one (six dummy in this case). The effect of the seventh day is captured in the intercept.

Let's see an example on Australian Beer Production data 
```{r}
library(fpp2)
beer2 = window(ausbeer, start = 1992)
autoplot(beer2) +
  xlab("Year") + ylab("Megalitres")
```

The quarterly data above shows seasonality and a decreasing trend.
We can model this using the equation :-
$$y_t = b_0 + b_1t + b_2d_{2,t} + b_3d_{3,t} + b_4d_{4,t}$$
$b_1$ is to capture the effect of trend. rest of the b's are to capture the effect of 3 quarters and effect of 1st quarter will be captured in intercept.
Coefficients associated with the other quarters are measures of the difference between those quarters and the first quarter.

```{r}
fit.beer = tslm(beer2 ~ trend + season)
summary(fit.beer)
```

There is an average negetive trend of -0.34 per quarter. On average, the second quarter has production of 34.7 megalitres lower than the first quarter, the third quarter has production of 17.8 megalitres lower than the first quarter, and the fourth quarter has production of 72.8 megalitres higher than the first quarter.

Let's see the fit of the model using some visualization.
```{r}
autoplot(beer2, series = "Data") +
  autolayer(fitted(fit.beer), series = "Fitted") +
  xlab("Year") + ylab("Megalitres") +
  ggtitle("Quarterly Beer Production")
```

```{r}
cbind(Data = beer2, Fitted = fitted(fit.beer)) %>% 
  as.data.frame() %>% 
  ggplot(aes(x = Data, y = Fitted, colour = as.factor(cycle(beer2)))) +
  geom_point() +
  ylab("Fitted") + xlab("Actual Values") +
  ggtitle("Quarterly Beer Production") +
  scale_color_brewer(palette = "Dark2", name = "Quarter") +
  geom_abline(slope = 1, intercept = 0)
```

**Intervention Variables :- **It is often necessary to model interventions that may have affected the variable to be forecast. For example, competitor activity, advertising expenditure, industrial action, and so on, can all have an effect.
When the effect lasts only for one period, we use a “spike” variable. "Spike" will takes value 1 in the period of intervention and zero elsewhere.
When the effect of intervention is permanent and immediate. If an intervention causes a level shift, we use a "step" variable. A step variable takes value zero before the intervention and one from the time of intervention onward.

There are other kind of intervention, that can be handled using some non linear equation.

**Trading Days :- **No. of trading days in a month can vary considerably and can have a substantial effect on the sales data. For that we can include no. of trading days as a variable.

**Distributed Lags :- **It is often useful to include advertising expenditure as a predictor. However, since the effect of advertising can last beyond the actual campaign, we need to include lagged values of advertising expenditure.

**Fourier Series :- **One issue with seasonal dummy variable is that if seasonal period are large then we have to create a lot of dummy variables. An alternate to that is to use Fourier series that is able to capture periodic function. Fourier series are combinations of sine and cosine functions.
```{r}
fourier.beer = tslm(beer2 ~ trend + fourier(beer2, K = 2)) #K = m/2, m = seasonal period
summary(fourier.beer)
```

A regression model containing Fourier terms is often called a harmonic regression.


# Selecting Predictors :-

Usually if the no. of predictors are less, one can plot the predictors along with the response variable to get an idea of the relationship but the relatioship of the variables might change in presence of other variable. Hence, this approach can not be used for selecting predictors.

Another approach that we need to ignore is to fit a multiple regression model using all the variable and reject variable that have a p value greater than 0.05 because p values are misleading when the variables are correlated.

Following the measures that can be used instead :-

**1. Adjusted $R^2$ :- ** We have seen earlier that one measure of goodness of fit is coefficient of determination $R^2$. Howerver, this is not a good measure of the preditive ability of the model. 

One issue with $R^2$ is that it does not allow for "degrees of freedom". Addition of any variable tend to increase $R^2$, even if that variable is irrelevent which leads to overfitting.
Also in case of simple linear regression, $R^2$ is the square of correlation coefficient of predictor and response variable only. So, if the model produces forecsats that are 20% of the actual value, then $R^2$ will be 1.

The same issues come with residual sum of squares. An alternate to that is adjusted $R^2$.
$$Adjusted R^2 = 1-(1-R^2) \frac{T-1}{T-k-1}$$
T = no. of observation.
k = no. of predictors.
This is an improvement on $R^2$, as it will no longer increase with each added predictor.

**2. Cross Validation :- **The procedure uses the following steps:
1. Remove observation t from the data set, and fit the model using the remaining data. Then compute the error ($e^{∗}_{t} = y_t − est(y_t)$) for the omitted observation. (This is not the same as the residual because the $t^{th}$ observation was not used in estimating the value of $y_t$.)
2. Repeat step 1 for t = 1,…,T..
3. Compute the MSE from $e^{∗}_{1},…,e^{∗}_{T}$. We shall call this the CV.
The best model is the one with the smallest value of CV.

**3. Akaike's Information Criterion :-** 
$$AIC = T*log(\frac{SSE}{T}) + 2(k+2),$$
where T = no. of observations.
k = no. of predictors.
The k+2 part of the equation occurs because there are k+2 parameters in the model: the k coefficients for the predictors, the intercept and the variance of the residuals.
The idea here is to penalise the fit of the model (SSE) with the number of parameters that need to be estimated.
The model with the minimum value of the AIC is often the best model for forecasting. For large values of T, minimising the AIC is equivalent to minimising the CV value.

**4. Corrected Akaike's Information Criterion :- **
$$AICc = AIC + \frac{2(k+2)(k+3)}{T-k-3}$$
For small values of T, AIC needs some correction as it selects model with too many predictors, and so a bias corrected version of AIC is AICc.

**5. Schwarz’s Bayesian Information Criterion :- **
$$BIC = T*log(\frac{SSE}{T}) + (k+2)log(T),$$
BIC penalises the number of parameters more heavily than the AIC. The model chosen by the BIC is either the same as that chosen by the AIC, or one with fewer terms.

If the value of T is very large then AIC, AICc, CV and BIC leads to same result.


## Stepwise Regression :-
If there are a large number of predictors, it is not possible to fit all possible models. An approach that works quite well is backwards stepwise regression:
* Start with the model containing all potential predictors.
* Remove one predictor at a time. Keep the model if it improves the measure of predictive accuracy.
* Iterate until no further improvement.

If the number of potential predictors is too large, then the backwards stepwise regression will not work and forward stepwise regression can be used instead. This procedure starts with a model that includes only the intercept. Predictors are added one at a time, and the one that most improves the measure of predictive accuracy is retained in the model. The procedure is repeated until no further improvement can be achieved.

Alternatively for either the backward or forward direction, a starting model can be one that includes a subset of potential predictors. In this case, an extra step needs to be included. For the backwards procedure we should also consider adding a predictor with each step, and for the forward procedure we should also consider dropping a predictor with each step. These are referred to as hybrid procedures.
