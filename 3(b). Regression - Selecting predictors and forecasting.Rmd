---
title: "3(b). Regression- selecting predictors and forecasting"
author: "Ankit Tyagi"
date: "1/17/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Fitting Trend :-**
Trend is common among time series. we usually visualize the data and get the idea of the trend. A linear trend can be fitted using the equation :- $$y_t = b_0 + b_1 t + e_t$$
where t = 1,2,....T. 

We can identify the non linear trend and fit a non linear equation as well.

**Fitting Seasonality :-** Suppose that we are forecasting daily data and we want to account for the day of the week as a predictor. Then we create a dummy variable for each day except one (six dummy in this case). The effect of the seventh day is captured in the intercept.

Let's see an example on Australian Beer Production data 
```{r}
library(fpp2)
beer2 = window(ausbeer, start = 1992)
autoplot(beer2) +
  xlab("Year") + ylab("Megalitres")
```

The quarterly data above shows seasonality and a decreasing trend.
We can model this using the equation :-
$$y_t = b_0 + b_1t + b_2d_{2,t} + b_3d_{3,t} + b_4d_{4,t}$$
$b_1$ is to capture the effect of trend. rest of the b's are to capture the effect of 3 quarters and effect of 1st quarter will be captured in intercept.
Coefficients associated with the other quarters are measures of the difference between those quarters and the first quarter.

```{r}
fit.beer = tslm(beer2 ~ trend + season)
summary(fit.beer)
```

There is an average negetive trend of -0.34 per quarter. On average, the second quarter has production of 34.7 megalitres lower than the first quarter, the third quarter has production of 17.8 megalitres lower than the first quarter, and the fourth quarter has production of 72.8 megalitres higher than the first quarter.

Let's see the fit of the model using some visualization.
```{r}
autoplot(beer2, series = "Data") +
  autolayer(fitted(fit.beer), series = "Fitted") +
  xlab("Year") + ylab("Megalitres") +
  ggtitle("Quarterly Beer Production")
```

```{r}
cbind(Data = beer2, Fitted = fitted(fit.beer)) %>% 
  as.data.frame() %>% 
  ggplot(aes(x = Data, y = Fitted, colour = as.factor(cycle(beer2)))) +
  geom_point() +
  ylab("Fitted") + xlab("Actual Values") +
  ggtitle("Quarterly Beer Production") +
  scale_color_brewer(palette = "Dark2", name = "Quarter") +
  geom_abline(slope = 1, intercept = 0)
```

**Intervention Variables :- **It is often necessary to model interventions that may have affected the variable to be forecast. For example, competitor activity, advertising expenditure, industrial action, and so on, can all have an effect.
When the effect lasts only for one period, we use a “spike” variable. "Spike" will takes value 1 in the period of intervention and zero elsewhere.
When the effect of intervention is permanent and immediate. If an intervention causes a level shift, we use a "step" variable. A step variable takes value zero before the intervention and one from the time of intervention onward.

There are other kind of intervention, that can be handled using some non linear equation.

**Trading Days :- **No. of trading days in a month can vary considerably and can have a substantial effect on the sales data. For that we can include no. of trading days as a variable.

**Distributed Lags :- **It is often useful to include advertising expenditure as a predictor. However, since the effect of advertising can last beyond the actual campaign, we need to include lagged values of advertising expenditure.

**Fourier Series :- **One issue with seasonal dummy variable is that if seasonal period are large then we have to create a lot of dummy variables. An alternate to that is to use Fourier series that is able to capture periodic function. Fourier series are combinations of sine and cosine functions.
```{r}
fourier.beer = tslm(beer2 ~ trend + fourier(beer2, K = 2)) #K = m/2, m = seasonal period
summary(fourier.beer)
```

A regression model containing Fourier terms is often called a harmonic regression.


# Selecting Predictors :-

Usually if the no. of predictors are less, one can plot the predictors along with the response variable to get an idea of the relationship but the relatioship of the variables might change in presence of other variable. Hence, this approach can not be used for selecting predictors.

Another approach that we need to ignore is to fit a multiple regression model using all the variable and reject variable that have a p value greater than 0.05 because p values are misleading when the variables are correlated.

Following the measures that can be used instead :-

**1. Adjusted $R^2$ :- ** We have seen earlier that one measure of goodness of fit is coefficient of determination $R^2$. Howerver, this is not a good measure of the preditive ability of the model. 

One issue with $R^2$ is that it does not allow for "degrees of freedom". Addition of any variable tend to increase $R^2$, even if that variable is irrelevent which leads to overfitting.
Also in case of simple linear regression, $R^2$ is the square of correlation coefficient of predictor and response variable only. So, if the model produces forecsats that are 20% of the actual value, then $R^2$ will be 1.

The same issues come with residual sum of squares. An alternate to that is adjusted $R^2$.
$$Adjusted R^2 = 1-(1-R^2) \frac{T-1}{T-k-1}$$
T = no. of observation.
k = no. of predictors.
This is an improvement on $R^2$, as it will no longer increase with each added predictor.

**2. Cross Validation :- **The procedure uses the following steps:
1. Remove observation t from the data set, and fit the model using the remaining data. Then compute the error ($e^{∗}_{t} = y_t − est(y_t)$) for the omitted observation. (This is not the same as the residual because the $t^{th}$ observation was not used in estimating the value of $y_t$.)
2. Repeat step 1 for t = 1,…,T..
3. Compute the MSE from $e^{∗}_{1},…,e^{∗}_{T}$. We shall call this the CV.
The best model is the one with the smallest value of CV.

**3. Akaike's Information Criterion :-** 
$$AIC = T*log(\frac{SSE}{T}) + 2(k+2),$$
where T = no. of observations.
k = no. of predictors.
The k+2 part of the equation occurs because there are k+2 parameters in the model: the k coefficients for the predictors, the intercept and the variance of the residuals.
The idea here is to penalise the fit of the model (SSE) with the number of parameters that need to be estimated.
The model with the minimum value of the AIC is often the best model for forecasting. For large values of T, minimising the AIC is equivalent to minimising the CV value.

**4. Corrected Akaike's Information Criterion :- **
$$AICc = AIC + \frac{2(k+2)(k+3)}{T-k-3}$$
For small values of T, AIC needs some correction as it selects model with too many predictors, and so a bias corrected version of AIC is AICc.

**5. Schwarz’s Bayesian Information Criterion :- **
$$BIC = T*log(\frac{SSE}{T}) + (k+2)log(T),$$
BIC penalises the number of parameters more heavily than the AIC. The model chosen by the BIC is either the same as that chosen by the AIC, or one with fewer terms.

If the value of T is very large then AIC, AICc, CV and BIC leads to same result.


## Stepwise Regression :-
If there are a large number of predictors, it is not possible to fit all possible models. An approach that works quite well is backwards stepwise regression:
* Start with the model containing all potential predictors.
* Remove one predictor at a time. Keep the model if it improves the measure of predictive accuracy.
* Iterate until no further improvement.

If the number of potential predictors is too large, then the backwards stepwise regression will not work and forward stepwise regression can be used instead. This procedure starts with a model that includes only the intercept. Predictors are added one at a time, and the one that most improves the measure of predictive accuracy is retained in the model. The procedure is repeated until no further improvement can be achieved.

Alternatively for either the backward or forward direction, a starting model can be one that includes a subset of potential predictors. In this case, an extra step needs to be included. For the backwards procedure we should also consider adding a predictor with each step, and for the forward procedure we should also consider dropping a predictor with each step. These are referred to as hybrid procedures.

/////////////////////////////////////////////////////\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\

When using regression models for time series data, we need to distinguish between the different types of forecasts that can be produced, depending on what is assumed to be known when the forecasts are computed.

## Ex-ante versus ex-post forecasts :-
These forecasts are made using only the information that is available in advance. Usually model requires forecasts of the predictors as they are not known in advance. Alternatively, forecasts from some other source, such as a government agency, may be available and can be used. Often, obtaining forecasts of the predictors can be challenging.
An alternative formulation is to use as predictors their lagged values. Assuming that we are interested in generating a h-step ahead forecast we write
$$y_{t+h} = b_0 + b_1 x_{1,t} + b_2 x_{2,t} + .....+ b_k x_{k,t} + e_{t+h}$$
for h = 1,2,.... This kind of model buliding works in many cases, for e.g, spend in marketing may not have an instantaneous effect on sales. It will happen with a lagging effect.

## Ex-post forecasts :-
These forecasts are made using later information on the predictors. These are not genuine forecasts, but are useful for studying the behaviour of forecasting models.
The model from which ex-post forecasts are produced should not be estimated using data from the forecast period. That is, ex-post forecasts can assume knowledge of the predictor variables, but should not assume knowledge of the data that are to be forecast.

One might want to use both type of forecast to determine the source of forecast uncertainity, whether forecast errors have arisen due to poor forecasts of the predictor or due to a poor forecasting model.


Now the value of the predictors are not known in advance so we cannot forecast in future but the special predictors that we have discussed above can be used for e.g, trend, sesonal dummy variable, spike variable etc as these variables are calender effects or the deterministic function of trend. In such cases, there is no difference between ex-ante and ex-post forecasts.

```{r}
beer2 <- window(ausbeer, start=1992)
fit.beer <- tslm(beer2 ~ trend + season)
fcast <- forecast(fit.beer)
autoplot(fcast) +
  ggtitle("Forecasts of beer production using regression") +
  xlab("Year") + ylab("megalitres")
```

## Scenario based Forecasting :-
In this setting, the forecaster assumes possible scenarios for the predictor variables that are of interest. For example, a US policy maker may be interested in comparing the predicted change in consumption when there is a constant growth of 1% and 0.5% respectively for income and savings with no change in the employment rate, versus a respective decline of 1% and 0.5%, for each of the four quarters following the end of the sample.

```{r}
fit.consBest = tslm(Consumption ~ Savings + Income + Unemployment,
                    data = uschange)
h = 4
newdata = data.frame(
  Income = c(1,1,1,1),
  Savings = c(0.5,0.5,0.5,0.5),
  Unemployment = c(0,0,0,0))
fcast.up = forecast(fit.consBest, newdata)

newdata <- data.frame(
    Income = rep(-1, h),
    Savings = rep(-0.5, h),
    Unemployment = rep(0, h))
fcast.down <- forecast(fit.consBest, newdata = newdata)

autoplot(uschange[,1]) +
  ylab("%change in US Consumption") +
  autolayer(fcast.up, PI = TRUE, series = "increase") +
  autolayer(fcast.down, PI = TRUE, series = "deccrease") +
  guides(colour = guide_legend(title = "Scenario"))
```

### Prediction Intervals :-
For a simple linear regression, Assuming that the regression errors are normally distributed, an approximate 95% prediction interval associated with this forecast is given by
$$est(y_t) \pm 1.96 \sigma_e \sqrt{1+\frac{1}{T} + \frac{(x-mean(x))^2}{(T-1)s_{x}^2}}, $$

```{r}
fit.cons <- tslm(Consumption ~ Income, data = uschange)
h <- 4
fcast.ave <- forecast(fit.cons,
  newdata = data.frame(
    Income = rep(mean(uschange[,"Income"]), h)))
fcast.up <- forecast(fit.cons,
  newdata = data.frame(Income = rep(5, h)))
autoplot(uschange[, "Consumption"]) +
  ylab("% change in US consumption") +
  autolayer(fcast.ave, series = "Average increase",
    PI = TRUE) +
  autolayer(fcast.up, series = "Extreme increase",
    PI = TRUE) +
  guides(colour = guide_legend(title = "Scenario"))
```

For Multiple linear regression, the prediction interval is 
$$est(y) \pm 1.96 σ_e \sqrt{1 + x^∗ (X^′X)^{−1} (x^∗)′}.$$

$x^∗$ be a row vector containing the values of the predictors for which we want to generate a forecast.
$$\mathbf{X} = \left[\begin{array}
{rrr}
1 & x_{1,1} & ... & x_{k,1} \\
1 & x_{1,2} & ... & x_{k,2} \\
1 & x_{1,3} & ... & x_{k,3} \\
. & . &. & .\\
. & . &. & .\\
. & . &. & .\\
1 & x_{1,T} & ... & x_{k,T} 
\end{array}\right]
$$

#### Excercise
2. Winning Times in Olympics men's 400m track final.

```{r}
autoplot(mens400) + 
  xlab("Year") + ylab("Time(in minutes)")
```

The data shows a decreasing trend over the years. Some missing observations are due to world war when Olympics were cancelled.
```{r}
tslm(mens400 ~ trend)
```

The winning time is decreasing at a rate of 0.25 minutes or 25 seconds.

```{r}
checkresiduals(tslm(mens400 ~ trend))
```

Autocorrelation is present in the data. Also there is an outlier present in the data in year 1895.
Also, there is some increasing trend can be seen in residuals after fitting the data.

```{r}
fit.mens = tslm(mens400 ~ trend)
forecast(fit.mens, h = 4)
```

5. Monthly Sales Figure of a shop
```{r}
autoplot(log(fancy/monthdays(fancy))) +
  xlab("Years") + ylab("Sales")
```

There is an increasing trend in the data with seasonal variation. Also, the variation in series is increasing with time. Also it seems that seasonal patterns are not consistent. 
Year 1991 shows a decrease in sales rather than increase.

The logarithmic transformation is required to control the varince first.
```{r}
surf_dummy = rep(0, length(fancy))
surf_dummy[cycle(fancy) == 3] = 1
surf_dummy[floor(time(fancy)) == 1987] = 0
fit.fancy = tslm(fancy ~ trend + season + surf_dummy, lambda = 0)
summary(fit.fancy)
```

```{r}
checkresiduals(fit.fancy)
```

Residuals are showing cyclic pattern, probably the log transformation was not good enough for the data. Same thing can be seen in the ACF plot as well.

```{r}
data.frame(residuals = residuals(fit.fancy),Month = cycle(fancy)) %>%
  ggplot(aes(x = Month, y = residuals, group = Month)) +
  geom_boxplot()
```

Box plot shows that the variation among residuals is not constant accross months. This might be because the seasonal patterns were not consistent and hence result in high residuals for some years.

```{r}
new_data = rep(0,36)
new_data = ts(data = new_data, start = 1994, end = c(1996,12), frequency = 12)
new_data[cycle(new_data) == 3] = 1
forecast(fit.fancy, newdata = data.frame(surf_dummy = new_data))
```


