---
title: "6(b).ARIMA Models - Foundation and Non Seasonal ARIMA"
author: "Ankit"
date: "3/7/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Autoregressive Model
Autoregressive model forecast the variable of interest using a linear combination of it's past values.

An autoregressive model of order p AR(p) is given as 
$$y_t = c + ϕ_1y_{t-1} + ϕ_2y_{t-2} +.......+ ϕ_py_{t-p} + e_t$$

Changing the parameters will result in different time series pattern. The variance of the error term will only change the scale of the series not the pattern.

Special cases of AR model:-

1. When $ϕ_1$ = 0, then $y_t$ is equivalent to white noise.

2. When $ϕ_1$ = 1 and c = 0, then $y_t$ is equivalent to the random walk.

3. When $ϕ_1$ = 1 and $c \ne 0$, then $y_t$ is equivalent to the random walk with drift.

4. When $ϕ_1$ < 0, $y_t$ tends to oscillate between positive and negative values;

We normally restrict autoregressive models to stationary data, in which case some constraints on the values of the parameters are required.

1. For an AR(1) model:- -1 < $ϕ_1$ < 1.

2. For an AR(2) model:- -1 < $ϕ_1$ < 1, $ϕ_1+ ϕ_2$  < 1  and $ϕ_2 - ϕ_1$<1.

When p $\ge$ 3, the restriction gets more complicated.

### Moving Average Model
A moving average model uses past forecast errors as predictors in a linear regression type model.

A moving average model with order q MA(q) is given as
$$y_t = c + e_t + \theta_1 e_{t-1} + \theta_2 e_{t-2} + ............ + \theta_qe_{t-q}$$
Note that it is not exactly a linear regression model as we do not observe $e_t$.

This is called moving average model because each $y_t$ can be think of as a weighted moving average of past forecast errors. It is different from moving average smoothing method as it trying forecast future values while MA smoothing tries to estimate trend cycle component.

It is possible to write a AR(p) model as MA(∞). The reverse is also true if we impose some constraints on the MA parameters. Then the MA model is called invertible.
For eg. Consider MA(1) process, in AR(∞) process, the most recent errors can be written as linear function of current and past observations.
$$e_t = \sum_{j=0}^{∞}(-\theta)^j y_{t-j}.$$
When |$\theta$| > 1, the weights increase as lags increase, so the more distant the observations the greater their influence on the current error. When |$\theta$| = 1, the weights are constant in size, and the distant observations have the same influence as the recent observations. As neither of these situations make much sense, we require |$\theta$| < 1, so the most recent observations have higher weight than observations from the more distant past. Thus, the process is invertible when |$\theta$| < 1.

1. For an MA(1) model:- -1 < $\theta_1$ < 1.

2. For an MA(2) model:- -1 < $\theta_2$ < 1, $\theta_2 + \theta_1$  < 1  and $\theta_1 - \theta_2$<1.

### Non-seasonal ARIMA Models
If we combine differencing with autoregression and a moving average model, we obtain a non seasonal ARIMA model. The full model can be written as
$$y_{t}^{'} = c + ϕ_1y_{t-1}^{'} + ϕ_2y_{t-2}^{'} +.......+ ϕ_py_{t-p} ^{'}+ \theta_1 e_{t-1} + \theta_2 e_{t-2} + ............ + \theta_qe_{t-q} + e_t$$

where $y_{t}^{'}$ is the differenced series. The above can be called as ARIMA(p,d,q). 

p = Order of autoregressive part.

d = degree of first differencing involved

q = order of moving average part.

Other models as special case of ARIMA models:-

1. ARIMA(0,0,0) = white noise.

2. ARIMA(0,1,0) with no constant = Random walk.

3. ARIMA(0,1,0) with a constant = Random walk with no drift.

4. ARIMA(p,0,0) = Autoregression.

5. ARIMA(0,0,q) = Moving average model.

Now let us denote B as a backshift operator, then $By_t = y_{t-1}$

So one-step difference can be written as $(1-B)y_t$. 

The ARIMA equation above can be written as
$$AR(p) = (1 - ϕ_1B - ϕ_2B^2 - .... ϕ_pB^p)$$
$$MA(q) = c + (1 + \theta_1B + \theta_2B^2 + .... + \theta_qB^q)e_t$$

then ARIMA equation is 
$$(1 - ϕ_1B - ϕ_2B^2 - .... ϕ_pB^p) (1-B)^dy_t = c+ (1+\theta_1B + \theta_2B^2 + .... + \theta_qB^q)e_t$$
Now selecting p,q,d can be very difficult, so we use the function auto.arima to do it.

**Example :-**
```{r}
library(fpp2)
autoplot(uschange[,"Consumption"]) +
  xlab("Year") + ylab("Quarterly percentage change")
```

```{r}
fit = auto.arima(uschange[,"Consumption"], seasonal = FALSE)
summary(fit)
```
This is a ARIMA(2,0,2) model.
$$y_t = c + 1.3908 y_{t-1} - 0.5813 y_{t-2} + e_t - 1.18 e_{t-1} + 0.5584 e_{t-2}$$
where c = 0.7463 * (1 - 1.3908 + 0.5813) = 0.1421701.
$e_t$ is the white noise with standard deviation of $\sqrt{0.3511} = 0.5925$.

Forecast is
```{r}
fit %>% forecast(h = 10) %>% autoplot(include = 80)
```

The constant c has an important effect on long term forecasts.

1. If c = 0 and d = 0, the long-term forecasts will go to zero.

2. If c = 0 and d = 1, the long-term forecasts will go to a constant.

3. If c = 0 and d = 2, the long-term forecasts will follow a straight line.

4. If c $\ne$ 0 and d = 0, the long-term forecasts will go to mean of the data.

5. If c $\ne$ 0 and d = 1, the long-term forecasts will follow a straight line.

6. If c $\ne$ 0 and d = 2, the long-term forecasts will follow a quadratic trend.

The value of d also has an effect on the prediction intervals — the higher the value of d, the more rapidly the prediction intervals increase in size. For d = 0, the long-term forecast standard deviation will go to the standard deviation of the historical data, so the prediction intervals will all be essentially the same.

The value of p is important if the data show cycles. To obtain cyclic forecasts, it is necessary to have p $\ge$ 2, along with some additional conditions on the parameters.

#### ACF and PACF plots
One way to find out the parameters p and q is ACF plots. ACF plots gives the correlation between lag values. 
One issue with ACF plot is if $y_t$ and $y_{t−1}$ are correlated, then $y_{t−1}$ and $y_{t−2}$ must also be correlated. However, then $y_t$ and $y_{t−2}$ might be correlated, simply because they are both connected to $y_{t−1}$, rather than any information contains in $y_{t-2}$ that could be used in forecasting $y_t$.

To overcome this we use partial autocorrelations. These measures the relationship between $y_t$ and $y_{t-k}$ after removing the effects of lags 1,2,3...., k-1. Each partial autocorrelation can be estiamted as the last coefficient in an autoregressive model.

Specifically, $α_k$, the kth partial autocorrelation coefficient, is equal to the estimate of $ϕ_k$ in an AR(k) model.

If the data are from ARIMA(0,d,q) or ARIMA(p,d,0) model, then the ACF and PACF plots can be helpful in determining the value of p or q. But if both p and q are positive, then the plots do not help in finding suitable values of p and q.

The data may follow an ARIMA(p,d,0) model if 

1. ACF is exponentially decaying or sinusoidal.

2. there is a significant spike at lag p in the PACF, but none beyond lag p.

The data may follow an ARIMA(0,d,q) model if 

1. PACF is exponentially decaying or sinusoidal.

2. there is a significant spike at lag q in the ACF, but none beyond lag q.

```{r}
ggAcf(uschange[,"Consumption"], main = "")
```

```{r}
ggPacf(uschange[,"Consumption"], main = "")
```

The PACF plot above shows that PACF plot has decreasing trend and ACF has a spike at 3 and then no other spike which shows that ARIMA(3,0,0) might work for this data.

```{r}
fit2 = Arima(uschange[,"Consumption"], order = c(3,0,0))
summary(fit2)
```

This model is giving a slightly better result then the model selected by auto.arima function.
This is because the auto.arima function do not use all the possible model in its search. 

```{r}
fit3 = auto.arima(uschange[,"Consumption"], seasonal = FALSE, stepwise = FALSE, approximation = FALSE)
summary(fit3)
```
This time it found the exact same model.

### Estimating the model

**Maximum likelihood estimation**
Once the model is decided (values of p, d and q), we estimate the parameters $c, ϕ_1, ϕ_2,....ϕ_p, \theta_1,..... \theta_q$ by maximizing the likelihood of the data. This technique is called maximum likelihood estimation (MLE). This technique finds the values of the parameters which maximise the probability of obtaining the data that we have observed.

**Information Criteria**
AIC is useful in selecting predictors for ARIMA model.
$$AIC = -2 log(L) + 2(p+q+k+1)$$
where L is the likelihood of the data.

Corrected AIC can be written as 
$$AICc = AIC + \frac{2(p+k+q+1)(p+k+q+2)}{T-p-q-k-2}$$
and Bayesian Information criteria as 

$$BIC = AIC + [log(T)-2](p+k+q+1)$$

How auto.arima work by defualt is 

1. Use KPSS test to determine the differencing ($0\le d \ge 2$) required for the data.

2. After differencing the data, choose the value of p and q by minimising AICc. To shorten this process the function uses a stepwise search approach. Four intial models are fitted :-
(I).ARIMA(0,d,0)
(II). ARIMA(2,d,2)
(III). ARIMA(1,d,0)
(IV). ARIMA(0,d,1)

A constant is included unless if d = 2, otherwise another model is fitted as ARIMA(0,d,0) without a constant.

3. The best model is selected that minimize AICc.

4. Variations on current model are considered like varying p and q by $\pm$ 1 and including/excluding constant from the current model.

5. The new model that minimise AICc becomes the best current model.

#### Modeling Procedure
See the chart "arimaflowchart" in the repository.

**Example **
Electrical Equipment orders 
We need to first adjust seasonality before we apply ARIMA.
```{r}
elecequip %>% stl(s.window = "periodic") %>% seasadj() -> eeadj
autoplot(eeadj)
```

The data is now seasonally adjusted and all we see is a cyclic variation. 
The data need not to be adjusted or transformed as variance is similar all over the time period.

The data is not stationary as there are ups and downs in the series for a long period of time. We will see if differencing can help solving that.
```{r}
eeadj %>% diff() %>% ggtsdisplay(main = "")
```

The differenced series looks stationary except there are some instance where we have high peaks and troughs. So we will not consider anymore differencing.

PACF shows that there are spike till lag 3 and no significant spike after that. ACF plot also spikes at 1 but after that no consistency and gives a confusing look.
We can try some iterations over ARIMA(3,1,0), ARIMA(3,1,1). Some other iterations can also be tried.

```{r}
fit = Arima(eeadj, order = c(3,1,0))
summary(fit)
```
```{r}
fit_2 = Arima(eeadj, order = c(3,1,1))
summary(fit_2)
```
ARIMA(3,1,1) is giving a slightly better result. 

```{r}
checkresiduals(fit_2)
```

The residuals from ARIMA(3,1,1) shows no significant autocorrelation. Also, the residual plot looks like a white noise. The LB test also suggest the same.

```{r}
autoplot(forecast(fit_2))
```

**Plotting the characteristic roots**
We can write ARIMA model equation as 
$$(1 - ϕ_1B - ϕ_2B^2 - .... ϕ_pB^p) (1-B)^dy_t = c+ (1+\theta_1B + \theta_2B^2 + .... + \theta_qB^q)e_t$$
$(ϕ)B = (1 - ϕ_1B - ϕ_2B^2 - .... ϕ_pB^p) $ is the pth order polynomial in B.

$(\theta)B = (1 - \theta_1B - \theta_2B^2 - .... \theta_pB^p) $ is the qth order polynomial in B.

The stationarity condition for the model requires the complex root of $(ϕ)B$ to be outside the unit circle and hence inverse root to be inside the unit circle.

The invertibility condition for the model requires the complex root of $(\theta)B$ to be outside the unit circle and hence inverse root to be inside the unit circle.

We can check that as
```{r}
autoplot(fit_2)
```

The inverse roots are inside the unit circle. If roots are near the unit circle or outside of it then the forecasts won't be stable.

#### Forecasts from ARIMA
Now we will see how ARIMA produce forecasts. We will consider the model that we have built above ARIMA(3,1,1).
The equation for this model can be written as 
$$(1 - ϕ_1B - ϕ_2B^2 - ϕ_3B^3) (1-B)y_t = (1+\theta_1B )e_t$$
The value of these parameters is in the output above. The above equation can be expanded as 
$$[1 - (1 + ϕ_1)B +(ϕ_1 + ϕ_2)B^2 + (ϕ_2 - ϕ_3)B^3 + ϕ_3B^4]y_t = (1 + \theta_1B)e_t$$ 
and replacing the backshift operator
$$y_t - (1 + ϕ_1)y_{t-1} +(ϕ_1 + ϕ_2)y_{t-2}+ (ϕ_2 - ϕ_3)y_{t-3} + ϕ_3y_{t-4} = e_t + \theta_1e_{t-1}$$ 
$$y_t = (1 + ϕ_1)y_{t-1} - (ϕ_1 + ϕ_2)y_{t-2} - (ϕ_2 - ϕ_3)y_{t-3} - ϕ_3y_{t-4} + e_t + \theta_1e_{t-1}$$
Now repalce t with T+1. $e_{T+1}$ will be zero and $e_T$ will be replaced with last observed residual.
$$y_{T+1|T} = (1 + ϕ_1)y_{T} - (ϕ_1 + ϕ_2)y_{T-1} - (ϕ_2 - ϕ_3)y_{T-2} - ϕ_3y_{T-3}  + \theta_1e_T$$
A forecast for T+2, and so on can be obtained in a similar way.

#### Excercise
**1. Generating Models**
To generate data from an AR(1) model with $ϕ_1 = 0.6$ and variance as 1. The process starts with $y_1$ = 0.
```{r}
y = ts(numeric(100))
e = rnorm(100)
for(i in 2:100)
  y[i] = 0.6*y[i-1] + e[i]
autoplot(y)
```

We will see how the plot changes with ϕ.
```{r}
ar1generator = function(phi){
  y = ts(numeric(100))
  e = rnorm(100)
  for(i in 2:100)
    y[i] = phi*y[i-1] + e[i]
  return(y)
}

autoplot(ar1generator(0.3), series = "0.3") +
  autolayer(ar1generator(0.6), series = "0.6") +
  autolayer(ar1generator(0.9), series = "0.9") +
  ylab("AR(1) Models") +
  guides(colour = guide_legend(title = "Phi1"))
```

As the value of phi increase, variation increases. This just represents that high value of phi represents that the models can take high adjustments. 

Now we will generate data from MA(1) process.
```{r}
ma1_generator = function(theta){
  y = ts(numeric(100))
  e = rnorm(100)
  for(i in 2:100)
    y[i] = theta * e[i-1] + e[i]
  return(y)
}  
autoplot(ma1_generator(0.3), series = "0.3") +
  autolayer(ma1_generator(0.6), series = "0.6") +
  autolayer(ma1_generator(0.9), series = "0.9") +
  ylab("MA(1) Models") +
  guides(colour = guide_legend(title = "theta1"))

```

The same conclusion as AR(1) model.
Now we will generate data from ARIMA(1,0,1) with phi = 0.6 and theta = 0.6.
```{r}
arima_101 = ts(numeric(50))
e = rnorm(50)
for(i in 2:50)
  arima_101[i] = 0.6*arima_101[i-1] + 0.6*e[i-1] + e[i]
autoplot(arima_101)
```

Now we will generate data from AR(2) with $ϕ_1$ = -0.8 and $ϕ_2$ = 0.3.
```{r}
ar2_generator = ts(numeric(100))
e = rnorm(100)
for(i in 3:100)
  ar2_generator[i] = -0.8 * ar2_generator[i-1] + 0.3 * ar2_generator[i-2] + e[i]
autoplot(ar2_generator)
```

```{r}
autoplot(arima_101, series = "ARMA(1, 1)") +
  autolayer(ar2_generator, series = "AR(2)") +
  ylab("y") +
  guides(colour = guide_legend(title = "Models"))
autoplot(arima_101)
```

Data from an AR(2) model increased with oscillation. They are non-staionary data. But data from an ARMA(1, 1) model were stationary.

**7. Fitting ARIMA Model**

We will explore wmurders (No. of women murders data)
```{r}
autoplot(wmurders) +
  ylab("No. of murders") + ggtitle("No. of women murders data")
```

The data shows no seasonality and no consistent trend, but the series is not stationary either. We can take a differencing and check if that makes it stationary.

```{r}
# wmurders %>% BoxCox(lambda = BoxCox.lambda(wmurders)) %>% diff() %>% ggtsdisplay()
wmurders %>% diff() %>% ggtsdisplay()
```

The data still do not look stationary. It shows some increasing variance with time. We can use ndiff function to find out the right differencing
```{r}
ndiffs(wmurders)
```
This shows that data need 2 differencing.
```{r}
wmurders %>% diff(differences = 2) %>% ggtsdisplay(main = "")
```

```{r}
library(urca)
wmurders %>% diff(differences = 2) %>% ur.kpss() %>% summary()
```

2nd differencing makes the data stationary. Now, PACF plot shows sigificant spike at lag 1. ACF plot also shows spikes at lag 1 and 2. We can start with ARIMA(1,2,0) or ARIMA(0,2,2) or we can try both onw by one and see if it makes the ACF and PACF plots more reasonable.

Now, as we are using 2nd order differencing, we should avoid using a constant in the model as it will yield quadratic trend, which might create problems while forecasting.

We can write ARIMA(1,2,0) as 
$$(1 - ϕ_1B) (1-B)^2y_t = e_t$$

```{r}
wmurders_arima_120 = Arima(wmurders, order = c(1,2,0))
summary(wmurders_arima_120)
```
We can also try ARIMA(0,2,2) model.
```{r}
wmurders_arima_022 = Arima(wmurders, order = c(0,2,2))
summary(wmurders_arima_022)
```

ARIMA(0,2,2) gives smaller AICc.
Now we will look at the residuals
```{r}
checkresiduals(wmurders_arima_022)
ggPacf(residuals(wmurders_arima_022))
```

The residuals shows mean as 0 but variance is not looking constant. ACF plot looks good. Also, PAcf plot do not show any spike either.

Let's see if auto.arima arrives at the same model.
```{r}
auto.arima(wmurders, seasonal = FALSE, stepwise = FALSE, approximation = FALSE)
```

ARIMA(0,2,3) gives better AICc and BIC. This means that there can be more than 1 model that satisfies the condtions that we target on residuals.

Now we will calculate the forecast manually.
Formula
$$y_t = 2y_{t-1} - y_{t-2} + e_t + \theta_1 e_{t-1} + \theta_2 e_{t-2}$$

```{r}
years = length(wmurders)
e = residuals(wmurders_arima_022)
fc1 = 2*wmurders[years] - wmurders[years-1] + -1.0181 * e[years] + 0.1470 * e[years-1]
fc2 = 2*fc1 - wmurders[years] + 0.1470 * e[years]
fc3 = 2*fc2 - fc1
c(fc1, fc2, fc3)
```

```{r}
forecast(wmurders_arima_022, h = 3)
```
Manually calculated forecasts are very similar to manually calculated forecast.
```{r}
forecast(wmurders_arima_022, h = 3) %>% autoplot()
```

**8. Fitting an ARIMA Model**

We will compare the effect of adding drift in the model.
Total international visitors to Australia (in millions)
```{r}
autoplot(austa) + 
  xlab("Year") + ylab("No. of visitors") +
  ggtitle("Total international visitors to Australia")
```

We will use auto.arima function to find the appropriate model.
```{r}
austa.arima = auto.arima(austa)
summary(austa.arima)
autoplot(forecast(austa.arima, h = 10))
```

ARIMA(0,1,1) with a constant. Let's see if the residuals looks like white noise.
```{r}
checkresiduals(austa.arima)
```

The residuals looks like white noise.
We can fit a simpler model with no drift anc compare the results with the above model.
```{r}
austa.arima_011 = Arima(austa, order= c(0,1,1))
autoplot(forecast(austa.arima_011 , h = 10))
```

We can see that a drift model obviously makes sense as there is a strong trend component in the model.

Let's remove the MA part as well and compare
```{r}
austa.arima_010 = Arima(austa, order= c(0,1,0))
autoplot(forecast(austa.arima_010 , h = 10))
```

The result from the last model is like naive forecasts. The forecast from the model with MA part are little higher as it is adjusting for the last forecasting as well.
The prediction interval from the 2nd model is higher that is probably because we are using one more parameter.

```{r}
austa.arima_213 = Arima(austa, order= c(2,1,3), include.drift = TRUE)
autoplot(forecast(austa.arima_213 , h = 10))
```

Now, in the above model as we have 2 AR and MA part. The constant will result in cubic trend.
The forecasts are increasing but the rate of increase is getting smaller with time.

Forecasts from an ARIMA(0,0,1) model with a constant.
```{r}
austa.arima_001 = Arima(austa, order= c(0,0,1), include.constant = TRUE)
autoplot(forecast(austa.arima_001 , h = 10))
```

The forecasts are fastly decreased to the mean of the data history. This happened at with increasing forecast horizon we make error terms zero.

```{r}
austa.arima_000 = Arima(austa, order= c(0,0,0), include.constant = TRUE)
autoplot(forecast(austa.arima_000 , h = 10))
```

If we remove the MA part as well. The forecasts will simply be the mean of the time series.

Forecasts from an ARIMA(0,2,1) model with no constant
```{r}
austa.arima_021 = Arima(austa, order= c(0,2,1), include.constant = TRUE)
autoplot(forecast(austa.arima_021 , h = 10))
```

The rate of increase in forecasts is very high. PI is being larger for the farther future forecast.

**ARIMA and ETS Comparison**

US GDP dataset
```{r}
autoplot(usgdp) + xlab("Year")
```

The data do not looks linear. We can use a BoxCox transformation and see if that makes it linear.
```{r}
autoplot(BoxCox(usgdp, lambda = BoxCox.lambda(usgdp))) 
```

The transformation makes the data linear. We will now use auto.arima to select a model.
```{r}
lambda_usgdp = BoxCox.lambda(usgdp)
usgdp_arima = auto.arima(usgdp, lambda = lambda_usgdp)
summary(usgdp_arima)
```

Let's check the residuals.

```{r}
checkresiduals(usgdp_arima)
```

We can look at ACF and PACF plot that will give us some idea how we can experiment with the paramters choosen.

```{r}
ggtsdisplay(diff(BoxCox(usgdp, lambda = lambda_usgdp)))
```

ACF plot shows 2 significant spikes and PACF shows one. We can experiment with the ARIMA(1,1,0) or ARIMA(0,1,2).

```{r}
usgdp_arima110 = Arima(usgdp, lambda = lambda_usgdp, order = c(1,1,0), include.drift = TRUE)
summary(usgdp_arima110)
checkresiduals(usgdp_arima110)
```

ACF plot still shows a spike at lag 2. Let's see the ARIMA(0,1,2)
```{r}
usgdp_arima012 = Arima(usgdp, lambda = lambda_usgdp, order = c(0,1,2), include.drift = TRUE)
summary(usgdp_arima012)
checkresiduals(usgdp_arima012)
ggPacf(residuals(usgdp_arima012))
```

The ARIMA(0,1,2) models also gives good enough results and we do not see any significant spike in ACF and PACF plots. There is a spike at lag 12 but that is difficult to explain as the data is quarterly. The RMSE is slightly higher but AICc and BIC are slightly smaller. 

```{r}
usgdp_ets = ets(usgdp)
summary(usgdp_ets)
```

```{r}
autoplot(usgdp) +
  autolayer(forecast(usgdp_arima012, h = 10), PI = FALSE, series = "ARIMA") +
  autolayer(forecast(usgdp_ets, h = 10), PI = FALSE, series = "ETS") +
  guides(colour = guide_legend(title = "Forecast"))
```

The foreasts coming from ETS seems more likely. We cannot compare ARIMA and ETS model using information value. They can be compared using cross validation method.

```{r}
fets = function(x,h){
  forecast(ets(x), h = h)
}

farima = function(x,h){
  forecast(auto.arima(x), h = h)
}
```


```{r}
#To compute the CV error
e1 = tsCV(usgdp, fets , h = 1)
e2 = tsCV(usgdp, farima , h = 1)
mean(e1^2, na.rm = TRUE)
```
```{r}
mean(e2^2, na.rm = TRUE)
```

ARIMA model gives smaller cross validation error.