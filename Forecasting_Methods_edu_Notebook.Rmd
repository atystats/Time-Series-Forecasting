---
title: "R Notebook for Forecasting"
output: html_notebook
---

''' 
#################### Simple Forecasting Methods #######################
1. Average Method :-
    a.Forecast of the future value is mean of the historical data.
    b.Time Series and Cross Sectional data.

2. Naive Method :-
    a. Forecast of future value is the last value in time series.
    b. Time series.

3. Seasonal Naive Method :-
    a. Highly seasonal data.
    b. Forecast set to last observed value from the same season last year.
    c. Time Series.

4. Drift Method :-
    a. Variation of the Naive method.
    b. Forecast set to increase/decrease over time.
    c. Change is the average change observed in the historical data.

Loading the package for forcasting

We are going to use the Australian Beer data from fpp package. The data is available from 1956 to 2008 for each quarter.
```{r}
library(fpp)
head(ausbeer)
```

```{r}
tail(ausbeer)
```

```{r}
plot(ausbeer)
# It is visible that there is an increasing trend over time.
# Also the data is seasonal. Seasonal pattern can be seen all over the plot.
```

```{r}
# We will consider the data from 1992 to 2005
beer2 = window(ausbeer, start = 1992, end = 2006 - .1)
head(beer2)
tail(beer2)
beer3 = window(ausbeer, start = 2006)
```

```{r}
# Forecasting using the Average Method
beerfit_avg = meanf(beer2, h = 11)
beerfit_avg
# In this case we are just taking the average of all the available values and using that as our forecast for future values.
```

```{r}
# Accuracy of the Method
accuracy(beerfit_avg,beer3)
print(paste("Root mean squared error is ",accuracy(beerfit_avg,beer3)[3]))
```

```{r}
# Forecasting using the Naive Method
beerfit_naive = rwf(beer2, h = 11)
beerfit_naive
# In this we are using the last value as our forecast i.e 2015 Q4 which is 482.
```

```{r}
# Accuracy of the Method
accuracy(beerfit_naive,beer3)
print(paste("Root mean squared error is ",accuracy(beerfit_naive,beer3)[3]))
```

```{r}
# Forecasting using the seasonal naive method
beerfit_snaive = snaive(beer2 , h = 11)
beerfit_snaive
# In this we are using the last value from each quarter and using them to forecast value for each upcoming quarters.
```
```{r}
# Accuracy of the Method
accuracy(beerfit_snaive,beer3)
print(paste("Root mean squared error is ",accuracy(beerfit_snaive,beer3)[3]))
```
Clearly Seasonal naive method has least root mean squared error.

Comaprison of the 3 methods graphically above
```{r}
plot(beerfit_avg,
     main = 'Forecast for quarterly beer production')
lines(beerfit_naive$mean, col = 2)
lines(beerfit_snaive$mean, col = 3)
lines(ausbeer)
legend("topright", lty = 1, col = c(4,2,3),
       legend = c("Mean Method", "Naive Method", "Seasonal Naive Method"))
```
The Seasonal Naive Method is the closest to the actual values which was expected as this is a seasonal data.

#############################################################################################
Now we are going to use a non seasonal data and see the performance of these models.

The data is Dow Jones Data

```{r}
# isntalling package for qrmdata
library(qrmdata)
dj = ts(DJ)
dj2 = window(dj, end = 250)
plot(dj)
```

```{r}
plot(dj2, main = "Dow Jones data daily",
     ylab = "", xlab = "Day", xlim = c(2,290), ylim = c(1240,1800))
lines(meanf(dj2, h = 42)$mean,col = 4)
lines(rwf(dj2, h = 42)$mean, col = 2)
lines(rwf(dj2,drift = TRUE, h = 42)$mean, col = 3)
lines(dj)
# drift method calculates the average change over time and that is our slope for forecasting.
```

###############################################################################################
###############################################################################################
###############################################################################################
###############################################################################################

Time Series Regression
We are going to use US Consumption data for this excerscise

```{r}
data("usconsumption")
head(usconsumption)
```
```{r}
ts.reg.fit = tslm(consumption ~ income ,data = usconsumption)
ts.reg.fit$coefficients

# This means that consumptions = 0.5206165 + 0.3186605 * income
```

```{r}
summary(ts.reg.fit)
```

We use time series regression for scenario based forecasting. For example, if we like to predict the consumption if income goes up 1 unit or decrease by 1 unit. This kind of forecasting is used mostly for insight rather than actually forecasting. We have to know the value of our predictor in advance.

```{r}
forecast(ts.reg.fit, newdata = data.frame(income = c(-1,1)))

```

##########################################################################################
##########################################################################################
Time Series Decomposition 
Time series pattern :-
1. Trend - Long term increase/decrease.
2. Seasonlity - Seasonal patterns influenced by seasonal patterns.
3. Cyclic Pattern - Rise and fall not of fixed period.
4. Distributed Lags.
5. Trading Days.

Let's look at a monthly housing data set.
```{r}
data("hsales")
plot(hsales, xlab = "year", ylab = "Monthly Housing Sales (millions)")
```

The plot above shows that within a year there is a increase and decrease in numbers and this is heppening in every which indicates that there is a seasonality present in the data.

Also, it can be seen that sales in 1975 is very low and then its is up for consecutive years till 1980 and drops again. This is an indication of a cyclic pattern in the data.

Let's look at another dataset of US treasury.

```{r}
data("ustreas")
plot(ustreas, xlab = "time", ylab = "US treasury bill contracts")
```

The above shows that there is downward trend over time but no clear indication of seasonality or cyclic variation.

Now we see Australian Monthly electricity production data
```{r}
data("elec")
plot(elec, xlab = "year", ylab = "Australian Monthly electricity production")
```

This plot shows that there is a upward trend present in the data. Also, seasonality can be seen in each year. But no cylclic pattern is visible.

This is how plotting a time series helps us in identifying the patterns exist in our data.
Note that in many cases the patterns are very clearly visible as we have seen in the above 2 charts.

Now we will try to decompose a time series.

We can write a time series model like :-
Additve Model :- Y = S + T + E
Multiplicative Model :- Y = S * T * E
Multiplicative model can be made additive by taking log on both sides.

Forecasting with decomposition :-

Now we will decompose a time series into 2 components
1. Seasonal Component.
2. Seasonally adjusted component.

Then we will fit each component seperately.

We are considering the dataset Electrical equipment Production

```{r}
data("elecequip")
head(elecequip)
# This dataset is available on a monthly scale.
plot(elecequip, xlab = "year", ylab = "Monthly Electrical Equipment Production")
```

```{r}
fit = stl(elecequip, t.window = 15, s.window = "periodic", robust = TRUE) 
# t.window is the time period to be considered for trend.
# s.window is for seasonality and periodic means that seasonality is present all over the time series.
fit
# the result will give us the trend, seasonality and the remainder for each observation.
```

Now lets remove seasonality from the data and fit the adjusted numbers using naive method.

```{r}
eeadj = seasadj(fit)
plot(naive(eeadj), xlab = "New orders index", 
     main = "Naive forecast of seasonally adjusted data")
```
 
```{r}
fcast = forecast(fit , method = "naive")
fcast
```

```{r}
plot(fcast, ylab= "New orders index")
```

In the above plot we can see that it has adjusted the naive method with seasonality.

##########################################################################################
##########################################################################################

Moving Average:- The forecast for current time is the average of last k time events.

Let's consider the Electricity Sales data :-
```{r}
data("elecsales")
plot(elecsales, ylab = "Electrical Sales Data", xlab = "Year")

# Now the 5 year moving average
lines(ma(elecsales, order = 5), col = "red")
```


Simple Exponential Smoothing :-
This method assumes that there is no trend or seasonality present in the data.
This method gives more weight to the recent observations and weight decrease over time in the past.

Let's fit exponential smoothing on oil Production data :-
```{r}
data("oil")
oildata = window (oil, start = 1996, end = 2007)
plot(oildata, ylab = "Oil (millions of tons)", xlab = "Year")
```

This data do not have any seasonal or cyclic varaition. Also, not any strong indication of a trend.

```{r}
fit1 = ses(oildata, alpha = 0.2 , initial = "simple", h = 3)
fit2 = ses(oildata, alpha = 0.6 , initial = "simple", h = 3)
fit3 = ses(oildata , h = 3)

plot(fit1, ylab = "Oil (Millions of tons)", xlab = "Year", type = "o", fcol = "white")
lines(fitted(fit1), col = "blue", type = "o")
lines(fitted(fit2), col = "red", type = "o")
lines(fitted(fit3), col = "green", type = "o")

lines(fit1$mean, col = "blue", type = "o")
lines(fit2$mean, col = "red", type = "o")
lines(fit3$mean, col = "green", type = "o")
legend("topleft", lty =1, col = c(1, "blue", "red", "green"),
      c("data", expression(alpha == 0.2), expression(alpha == 0.6), 
        expression(alpha == 0.89)), pch = 1)
```

It can be seen that increasing the alpha just increasing the flexibility of the model.

##################################################################################################################
##################################################################################################################

ARIMA Modeling :- This modeling aims to explain the suto correlation in the data.

Two components of ARIMA are :-
1. Auto regressive :- This means that we are regressing a variable on its own past value.
2. Moving Average.

Before start on ARIMA, let's understand some more terms in our mind.
Stationary = Time series that are independent of time are called stationary time series.

Time series with trend and seasonality factors are not stationary.

Time series with cyclic variation are stationary.

Stationary Series are not predictable.

Some ways of making a time series stationary:-

Differencing :- Calculating the difference between consecutive observation points.
Differencing removes trend and seasonality.

Transformation the variable. For eg.  log transformation.

############### Auto Regressive Model ###################

Auto Regressive Model - Forecast variable of interest using linear combination of the past values of the variable of interest.

AR(p) = A + Y(t-1) + Y(t-2) + ...... + Y(t-p) + E
where p is the order of auto regressive.

Special Cases of AR(1) Model.
1. if b = 0 then y = a (This is called white noise as the variation in forecasted values is because of the random error term).
2. if b = 1 and a = 0 then y = y(t-1) (This is called random walk as the forecasted value is just based on the last value).
3. if b = 1 and a!= 0 then y = a + y(t-1) (This is called random walk with drift).

############## Moving Average Model ##################

These are regression like model using past forecast errors. We use the weighted average of the past forecast errors.

MA(p) = A + Et + E(t-1) + E(t-2) +...... + E(t-p).
p is the order of moving average.

############## Non Seasonal ARIMA models ###############
Differencing with Auto Regressive + Moving Average Model.

Yt = A + Y'(t-1) + Y'(t-2) +.....+ Y'(t-p) + Et + E(t-1) + E(t-2) +.....+ E(t-q)

ARIMA(p,d,q)
p = order of the autoregressive part.
q = order of the moving average.
d = degree of the differencing involved.

auto.arima function can be used to determine these parameters
```{r}
plot(usconsumption[,1])

```

```{r}
# Now lets use the auto.arima function
fit = auto.arima(usconsumption[,1], seasonal = FALSE)

fit 
```

This means that a MA(3) model is best for this dataset.

```{r}
fit = Arima(usconsumption[,1], order = c(0,0,3))
```

The paramters can also be decided using the ACF and PACF plots.
ACF = Auto Correlation Factor. This represents the correlation between points seperated by time period.
      ACF can be used to decide the q of the ARIMA model.
PACF = Partial auto correlation Factor. This measure the partial correlation.
Y(t) can be correlated to Y(t-2) because both are correlated to Y(t-1).

```{r}
par(mfrow = c(1,2))
Acf(usconsumption[,1])
Pacf(usconsumption[,1])
```

The PACF plot is exponentially decreasing but there is a spike at lag 3 in ACF plot. So the best model is ARIMA(0,0,3).

Now we will fit a ARIMA model on Electrical Equipment data
```{r}
plot(elecequip, xlab = "year", ylab = "Monthly Electrical Equipment Production")
eeadj = seasadj(stl(elecequip, s.window = "periodic"))
```

```{r}
## Seasonally adjusted time series
plot(eeadj)
```

```{r}
# We can plot all the 3 plots in 1 window
tsdisplay(diff(eeadj))
```



```{r}
auto.arima(eeadj)
```

```{r}
fit = Arima(eeadj, order = c(3,1,1))
summary(fit)
```

```{r}
fit = Arima(eeadj, order = c(3,1,0))
summary(fit)
```
  
```{r}
fit = Arima(eeadj, order = c(4,1,0))
summary(fit)
```

```{r}
Acf(residuals(arima(eeadj, order = c(3,1,1))))
```

```{r}
plot(forecast(arima(eeadj, order = c(3,1,1))))
```

Seasonal ARIMA Models 

```{r}
data("euretail")
plot(euretail)
```

```{r}
tsdisplay(diff(euretail,4))
```

```{r}
fit = Arima(euretail, order = c(0,1,1), seasonal = c(0,1,1))
tsdisplay(residuals(fit))
```

```{r}
fit = Arima(euretail, order = c(0,1,3), seasonal = c(0,1,1))
tsdisplay(residuals(fit))
```

```{r}
plot(forecast(fit, h = 12))
```
```{r}
auto.arima(euretail)
```

