---
title: "7(b). Dynamic harmonic Regression"
author: "Ankit"
date: "4/9/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

When there are long sesonal periods, a regression with fourier terms can be used as ARIMA and ETS models are designed for shorter periods. Also, for example for daily level data , it does not make much sense to compare what happened today to what happened same day last year and there is no constraint that the seasonal pattern is smooth.

So for such time series, we prefer a harmonic regression approach where the seasonal pattern is modelled using Fourier terms with short-term time series dynamics handled by an ARMA error. The disadvantage is that we assume seasonality is fixed which might not be true especially for long time series.

There are some advantages of this approach other than it allows any length of seasonality.

1. For data more than one seasonality, Fourier terms of different frequencies can be included.

2. the smoothness of the seasonal patterns can be controlled with K, the seasonal pattern is smoother for smaller values of K.

3. the short term dynamics can be handled easily with a simple ARMA error.

**Example**
Australian eating out expenditure
The data is avaliable on a monthly level.
```{r}
library(fpp2)
autoplot(auscafe) + xlab("Year") +
  ylab("Eating out expenditure in billions")
```

The data seasonal in nature and the seasonality is increasing with the level of the series. We might have to take a log of the series.
Now we will explain these seasonal variation using Fourier terms, with varying the value of K.
```{r}
cafe = window(auscafe, start = 2004)
plots = list()
for (i in 1:6) {
  fit = auto.arima(cafe, xreg = fourier(cafe, K = i),
                   seasonal = FALSE, lambda = 0)
  plots[[i]] = autoplot(forecast(fit, 
                                 xreg = fourier(cafe, K = i, h = 24))) +
    xlab(paste("K=",i,"   AICC=",round(fit[["aicc"]],2))) +
    ylab("") + ylim(1.5,4.7)
}

gridExtra::grid.arrange(
  plots[[1]],plots[[2]],plots[[3]],
  plots[[4]],plots[[5]],plots[[6]], nrow = 2)
```

As we can see, increasing the value of K, we are able to capture more wiggly pattern as well. K = 5 is giving us the best results. So, we are using smaller value of k. If we use K = 11, that would be equivalent to creating 11 dummy variables.

#### Lagged Predictors
Some we need to include lagged values of a variable simple because these variables do not have an instant effect but their effect can be seen after certain period of time. For eg. increase in marketing spend might impact sales for coming months than the current month.

Now how many lagged values we need to use for a predictor can be decide using AICc.

**Example**
Impact of TV advertising on insurance quotations.
```{r}
autoplot(insurance, facets = TRUE) +
  xlab("Year") + ylab("") +
  ggtitle("Insurance advertising and quotations")
```

In this model, we will include the expenditure for up to 4 months.

```{r}
Advert = cbind(
  AdLag0 = insurance[,"TV.advert"],
  AdLag1 = lag(insurance[,"TV.advert"],-1),
  AdLag2 = lag(insurance[,"TV.advert"],-2),
  AdLag3 = lag(insurance[,"TV.advert"],-3)) %>% 
  head(NROW(insurance))

# We need to exclude the first 3 rows to make a fair comparison
fit1 = auto.arima(insurance[4:40,1], xreg = Advert[4:40,1],
                  stationary = TRUE)
fit2 = auto.arima(insurance[4:40,1], xreg = Advert[4:40,1:2],
                  stationary = TRUE)
fit3 = auto.arima(insurance[4:40,1], xreg = Advert[4:40,1:3],
                  stationary = TRUE)
fit4 = auto.arima(insurance[4:40,1], xreg = Advert[4:40,1:4],
                  stationary = TRUE)
c(fit1[["aicc"]],fit2[["aicc"]],fit3[["aicc"]],fit4[["aicc"]])
```

The best model is with 2 lag predictors. Now we can use all the data to fit this model.
```{r}
(fit = auto.arima(insurance[,1], xreg = Advert[,1:2],
                  stationary = TRUE))
```

The model that is selected is AR(3) model. This model can be written as
$$y_t = 2.0393 + 1.2564 x_t + 0.1625 x_{t-1} + \eta_t$$
$$\eta_t = 1.4117 \eta_{t-1} - 0.9317 \eta_{t-2} + 0.3591 \eta_{t-3} + \epsilon_t$$
and $\epsilon_t$ is a white noise process.

Forecast from the above model
```{r}
fc = forecast(fit, h = 20,
              xreg = cbind(AdLag0 = rep(8,20),
                           Adlag1 = c(Advert[40,1],rep(8,19))))
autoplot(fc) + xlab("Years") +
  ylab("Quotes") +
  ggtitle("Forecast quotes with future advertising set to 8")
```

