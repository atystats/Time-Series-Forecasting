---
title: "7(a). Dynamic Regression - Intro"
author: "Ankit"
date: "4/5/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

ARIMA Models or state space models allows for including information from past observations of a series, but do not allow inclusion of other informations like effect of hoildays, promo activities, competitior activity etc. 

On the other hand, Time series models can take care of these other relevant information but do not include past observations as input.

**Dynamic Regression** is an approach to extend ARIMA models in order to allow other information to be included in the models.
We can write a dynamic regression model as 
$$y_t = \beta_0 + \beta_1 x_{1,t} +.....+ \beta_kx_{k,t} + \eta_t$$
where $\eta_t$ follows an ARIMA model instead of being a white noise.

The challange with these type of models is that we cannot least square kind of methods where we minimise error term (in this case $\eta_t$) to find the estimates. This happens because of several reasons.

1. The estimated coefficients are no longer the best estimates as some information has been ignored, which we have considered in the ARIMA model.

2. As the $\eta_t$ do not follow white noise distribution but an ARIMA Model, all the t-test will be insignificant.

3. AICc or BIC cannot be used in most of the cases to choose the best model.

4. p-values may not be valid in most of the cases. Usually the p-value gets smaller.

Alternatively, we will use maximum likelihood estimation.

An important consideration when estimating a regression with ARMA errors is that all of the variables in the model must first be stationary. Thus, we first have to check that $y_t$ and all of the predictors $(x_{1,t},â€¦, x_{k,t})$ appear to be stationary. If we estimate the model when any of these are non-stationary, the estimated coefficients will not be consistent estimates.  One exception to this is the case where non-stationary variables are co-integrated. If there exists a linear combination of the non-stationary $y_t$ and the predictors that is stationary, then the estimated coefficients will be consistent.

So, we first difference the non-stationary variables. Now, we often need to maintain the form of relationship in between $y_t$ and the predictors, and consequently it is common to difference all the variables if any of them needs differencing. The resulting model is called a "model in differences". 

If all of the variables in the model are stationary, then we only need to consider ARMA errors for the residuals. It is easy to see that a regression model with ARIMA errors is equivalent to a regression model in differences with ARMA errors.

The model above with differencing can be written as
$$y_{t}^{'} = \beta_0 + \beta_1 x_{1,t}^{'} +.....+ \beta_kx_{k,t}^{'} + \eta_{t}^{'}$$
where $y_{t}^{'} =  y_{t} - y_{t-1}$, $x_{t,i}^{'} =  x_{t,i} - x_{t-1,i}$ and $\eta_{t}^{'} = \eta_t - \eta_{t-1}$.

**Example**
US Personal Consumption and Income
```{r}
library(fpp2)
autoplot(uschange[,1:2], facets = TRUE) +
  xlab("Year") + ylab("") +
  ggtitle("Quarterly changes in US consumption
    and personal income")
```

The data looks stationary so there is no need for differencing. We can use auto.arima to fit a dynamic regression model.
```{r}
fit = auto.arima(uschange[,"Consumption"],
                 xreg = uschange[,"Income"])
summary(fit)
```
The fitted model is 
$$y_t = 0.599 + 0.2028x_t + \eta_t$$
$$\eta_t = 0.6922\eta_{t-1} + \epsilon_t - 0.5758 \epsilon_{t-1} + 0.198 \epsilon_{t-2}$$

Now we will plot both ARIMA and regression errors
```{r}
cbind("Regression Errors" = residuals(fit, type = "regression"),
      "ARIMA Errors" = residuals(fit, type = "innovation")) %>% 
  autoplot(facets = TRUE)
```

It is the ARIMA errors that should resemble a white noise series.
```{r}
checkresiduals(fit)
```

To forecast using a regression model with ARIMA errors, we need to forecast the regression part of the model and the ARIMA part of the model, and combine the results.

In above example we are assuming that future percentage change in income is simply the average of the income in last 40 years.
```{r}
fcast = forecast(fit, xreg = rep(mean(uschange[,"Income"]),8))
autoplot(fcast) + xlab("Year") +
  ylab("Percentage change")
```

The prediction that are calculated do not include the uncertainity associated with the income variable itself. We assume that we know the value of predictors prior forecasting.

**Example 2. Forecasting electricity demand**
```{r}
qplot(Temperature, Demand, data = as.data.frame(elecdaily)) +
  xlab("Temperature") + ylab("Demand")
```

It is clear that the relationship between demand and temperature is non-linear. More electricity is used on cold days due to heating and hot days due to air conditioning.
We have a indicator that mark as workday. It takes value 1 on workday and 0 otherwise.
```{r}
autoplot(elecdaily[,c(1,3)], facets = TRUE, )
```


The plot shows that we need a non-linear and dynamic model. We saw a U-shape in the above graph, so we will a quadratic regression model.

```{r}
xreg = cbind(MaxTemp = elecdaily[,"Temperature"],
      MaxTempsq = elecdaily[,"Temperature"]^2,
      Workday = elecdaily[,"WorkDay"])
fit = auto.arima(elecdaily[,"Demand"], xreg = xreg)
summary(fit)
checkresiduals(fit)
```

The model some significant correlation, Also, we can see some ouliers in the residuals which will effect the prediction intervals.

Using the estimated model we forecast 14 days ahead starting from Thursday 1 January 2015 (a non-work-day being a public holiday for New Years Day). We assume temperature for the next 14 days to a constant 26 degrees.
```{r}
fcast = forecast(fit, xreg = cbind(rep(26,14),
                                   rep(26^2,14),
                                   c(0,1,0,0,1,1,1,1,1,0,0,1,1,1)))
autoplot(fcast) + ylab("Electricity demand (GW)")
```

#### Trends in Dynamic Regression
There are 2 kind of trends. A deterministic trend can be modelled as
$$y_t = \beta_0 + \beta_1t + \eta_t,$$
where $\eta_t$ is an ARMA process. A stochastic process is 
$$y_t = \beta_0 + \beta_1t + \eta_t,$$
where $\eta_t$ is an ARIMA process with d = 1. We can take differences both the sides so that 
$$y_{t}^{'} = \beta_1 + \eta_{t}^{'}$$
where $\eta_{t}^{'}$ is an ARIMA process. In other words,
$$y_t = y_{t-1} + \beta_1 + \eta_{t}^{'}$$

**Example - International visitors to Australia**
```{r}
autoplot(austa) + xlab("Year") +
  ylab("millions of people") +
  ggtitle("Total annual international visitors in Australia")
```

We will first fit deterministic trend to the data.
```{r}
trend = seq_along(austa) 
(fit1 = auto.arima(austa, d = 0, xreg = trend))
```

The model selected is 
$$y_t = 0.4156 + 0.171t + \eta_t$$
$$\eta_t = 1.1127 \eta_{t-1} - 0.3805 \eta_{t-2} + \epsilon_t $$
and $\epsilon_t$ follows NID(0, 0.2979).
The estimated growth in visitor numbers is 0.17 million people per year.

Now, we will fit stochastic trend to the data.
```{r}
(fit2 = auto.arima(austa, d = 1))
```
The model selected is 
$$y_t -y_{t-1} = 0.1735 + \eta_{t}^{'}$$
$$y_t = y_0 + 0.1735t + \eta_t$$
$$\eta_t = \eta_{t-1} + 0.3006 \epsilon_{t-1} + \epsilon_t$$
and $\epsilon_t$ follows NID(0, 0.03376).
In this case, the estimated growth in visitor numbers is also 0.17 million people per year. But the prediction interval from both the models won't be similar.
```{r}
fc1 = forecast(fit1,
               xreg = cbind(trend = length(austa) + 1:10))

fc2 = forecast(fit2, h = 10)
autoplot(austa) +
  autolayer(fc2, "Stochastic trend") +
  autolayer(fc1, "Deterministic trend") +
  ggtitle("Forecasts from trend models") +
  xlab("Year") + ylab("Visitors to Australia (millions)") +
  guides(colour = guide_legend(title = "Forecast"))
```

In deterministic trends the slope of the trend is not going to change over time. On the other hand, stochastic trends can change, and the estimated growth is only assumed to be the average growth over the historical period, not necessarily the rate of growth that will be observed into the future. 

It is safer to forecast with stochastic trends, especially for longer forecast horizons, as the prediction intervals allow for greater uncertainty in future growth.

**Excercise**
**1. Monthly Sales and advertising expenditure for an automotive parts company.**
```{r}
autoplot(advert, facets = TRUE) +
  xlab("Year") + ylab("Sales and advertising expenditure")
```

We will first fit a linear regression model. 
```{r}
fit_linear = tslm(sales ~ advert, data = advert)
summary(fit_linear)
```
The fitted model is $y_t = 78.73426 + 0.53426 * x_t + \epsilon_t$.
```{r}
checkresiduals(fit_linear)
```

The residuals shows significant correlation in lag 1 and lag 2.
```{r}
fit_arima = Arima(advert[,"sales"], xreg = advert[,"advert"],
      order = c(0,0,0))
checkresiduals(fit_arima)
```

Now we will fit model using auto.arima function.
```{r}
(fit_arima2 = auto.arima(advert[,"sales"], xreg = advert[,"advert"]))
checkresiduals(fit_arima2)
```

The selected error structure for the model is ARIMA(0,1,0). The residuals do not show any significant correlation. Also, the residuals looks like white noise.
```{r}
fc = forecast(fit_arima2, h = 6, xreg = rep(10,6))
autoplot(fc)
```

**Exercise 2. Level of Lake Huron**
```{r}
autoplot(huron) 
```

We will fit a piecewise regression with a knot at 1920 as there is a change in slope of trend in the time series.
```{r}
trend = time(huron)
trend_knot = ts(pmax(0, trend - 1920), start = time(huron)[1])
(fit.pw = auto.arima(huron, xreg = cbind(trend, trend_knot)))
```

Regression with AR(2) errors.
```{r}
autoplot(huron) +
  autolayer(fit.pw$fitted)
```

Forecast from the model.
```{r}
t.fc = trend[length(trend)] + seq(30)
t.knot.fc = trend_knot[length(trend)] + seq(30)
fc = forecast(fit.pw, xreg = cbind(t.fc, t.knot.fc), h = 30)
autoplot(fc)
```

**Exercise 3. Total accommodation at hotel, motel and guest house**
```{r}
autoplot(motel, facets = TRUE)
```

```{r}
avg.cost = motel[,"Takings"] / motel[,"Roomnights"]
autoplot(avg.cost) + 
  xlab("Year") + ylab("Average Accomodation Cost")
```

The average accomodation cost is increasing at first and then stabilize after year 1990.
```{r}
(cpi_fit = auto.arima(motel[,"CPI"],lambda = 0))
```

We have a ARIMA(0,2,1) model.
```{r}
autoplot(cpimel) +
  autolayer(cpi_fit$fitted,series = "Fitted Value")
```

Both the variables shows increase in varaition with time and a transformation might make it more linear.

```{r}
(fit.cost = auto.arima(avg.cost, xreg = cpi_fit$fitted, lambda = 0, approximation = FALSE))
```

Forecast for this model. We need to forecast for CPI for next 12 years.
```{r}
fc_cpi = forecast(cpi_fit, h = 12)
fc_cost = forecast(fit.cost, xreg = fc_cpi$mean, h = 12)
autoplot(fc_cost)
```

**Excercise 4 - US finished motor gasoline product supplied**
```{r}
autoplot(gasoline)
```

The data show increasing trend till 2007, then a slightly downward trend till 2012-13, then increasing again.
```{r}
t = time(gasoline)
t.knot1 = 2007
t.knot2 = 2013
t.pw1 = ts(pmax(0,t - t.knot1), start = t[1], frequency = 365.25/7)
t.pw2 = ts(pmax(0,t - t.knot2), start = t[1], frequency = 365.25/7)
AICc <- Inf
K_min.Aicc <- 0
# use for-loop to find the harmonic regression model which yields least AICc. Maximum number of repetition is 26 times because the maximum number of pairs should be less than a half of the number of weeks in a year, 52.18.
for(num in c(1:26)){
  gasoline_tslm <- tslm(
    gasoline ~ trend + t.pw1 + t.pw2 + fourier(
      gasoline, K = num
    )
  )
  AICc_value <- CV(gasoline_tslm)["AICc"]
  
  if(AICc > AICc_value){
    AICc <- AICc_value
  }else{
    K_min.Aicc <- num
    break 
  }
}
K_min.Aicc
```
```{r}
gasoline_tslm
```

```{r}
gasoline_arima = auto.arima(gasoline, xreg = cbind(t = t,t.pw1 = t.pw1,t.pw2 = t.pw2, Fourier = fourier(gasoline, K = 11)))
```
```{r}
gasoline_arima
checkresiduals(gasoline_arima)
```

```{r}
gasoline.from2000 <- window(gasoline, start = 2000)
t.from2000 <- window(t, start = 2000)
t.pw1.from2000 <- window(t.pw1, start = 2000)
t.pw2.from2000 <- window(t.pw2, start = 2000)
# find the number of Fourier pairs for new data. 
AICc <- Inf
K_min.Aicc <- 0
for(num in c(1:26)){
  gasoline.from2000_tslm <- tslm(
    gasoline.from2000 ~ trend + t.pw1.from2000 + t.pw2.from2000 + fourier(
      gasoline.from2000, K = num
    )
  )
  AICc_value <- CV(gasoline.from2000_tslm)["AICc"]
  
  if(AICc > AICc_value){
    AICc <- AICc_value
  }else{
    K_min.Aicc <- num
    break 
  }
}
K_min.Aicc
# still 11 Fourier pairs were chosen.
gasoline.from2000_tslm
```

```{r}
xreg.from2000 <- cbind(
  t = t.from2000, 
  t.pw1 = t.pw1.from2000, 
  t.pw2 = t.pw2.from2000,
  Fourier = fourier(
    gasoline.from2000, K = 11
    )
  )
# It also takes some minutes to run.
gasoline.from2000_autoarima <- auto.arima(
  gasoline.from2000,
  xreg = xreg.from2000
)
```
```{r}
gasoline.from2000_autoarima
checkresiduals(gasoline.from2000_autoarima)
```

The residuals do not looks like white noise. We can probably add more autoregressive terms to control autocorrelation. Adding more parameters will compromise on the likelihood.
```{r}
gasoline.from2000_arima.6.0.1 <- Arima(
  gasoline.from2000,
  xreg = xreg.from2000,
  order = c(6, 0, 1)
)
checkresiduals(gasoline.from2000_arima.6.0.1)
```

ACF plots looks much better. The residuals looks more normal but the residuals do not have constant variance.