---
title: "7(a). Dynamic Regression - Intro"
author: "Ankit"
date: "4/5/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

ARIMA Models or state space models allows for including information from past observations of a series, but do not allow inclusion of other informations like effect of hoildays, promo activities, competitior activity etc. 

On the other hand, Time series models can take care of these other relevant information but do not include past observations as input.

**Dynamic Regression** is an approach to extend ARIMA models in order to allow other information to be included in the models.
We can write a dynamic regression model as 
$$y_t = \beta_0 + \beta_1 x_{1,t} +.....+ \beta_kx_{k,t} + \eta_t$$
where $\eta_t$ follows an ARIMA model instead of being a white noise.

The challange with these type of models is that we cannot least square kind of methods where we minimise error term (in this case $\eta_t$) to find the estimates. This happens because of several reasons.

1. The estimated coefficients are no longer the best estimates as some information has been ignored, which we have considered in the ARIMA model.

2. As the $\eta_t$ do not follow white noise distribution but an ARIMA Model, all the t-test will be insignificant.

3. AICc or BIC cannot be used in most of the cases to choose the best model.

4. p-values may not be valid in most of the cases. Usually the p-value gets smaller.

Alternatively, we will use maximum likelihood estimation.

An important consideration when estimating a regression with ARMA errors is that all of the variables in the model must first be stationary. Thus, we first have to check that $y_t$ and all of the predictors $(x_{1,t},â€¦, x_{k,t})$ appear to be stationary. If we estimate the model when any of these are non-stationary, the estimated coefficients will not be consistent estimates.  One exception to this is the case where non-stationary variables are co-integrated. If there exists a linear combination of the non-stationary $y_t$ and the predictors that is stationary, then the estimated coefficients will be consistent.

So, we first difference the non-stationary variables. Now, we often need to maintain the form of relationship in between $y_t$ and the predictors, and consequently it is common to difference all the variables if any of them needs differencing. The resulting model is called a "model in differences". 

If all of the variables in the model are stationary, then we only need to consider ARMA errors for the residuals. It is easy to see that a regression model with ARIMA errors is equivalent to a regression model in differences with ARMA errors.

The model above with differencing can be written as
$$y_{t}^{'} = \beta_0 + \beta_1 x_{1,t}^{'} +.....+ \beta_kx_{k,t}^{'} + \eta_{t}^{'}$$
where $y_{t}^{'} =  y_{t} - y_{t-1}$, $x_{t,i}^{'} =  x_{t,i} - x_{t-1,i}$ and $\eta_{t}^{'} = \eta_t - \eta_{t-1}$.

**Example**
US Personal Consumption and Income
```{r}
library(fpp2)
autoplot(uschange[,1:2], facets = TRUE) +
  xlab("Year") + ylab("") +
  ggtitle("Quarterly changes in US consumption
    and personal income")
```

The data looks stationary so there is no need for differencing. We can use auto.arima to fit a dynamic regression model.
```{r}
fit = auto.arima(uschange[,"Consumption"],
                 xreg = uschange[,"Income"])
summary(fit)
```
The fitted model is 
$$y_t = 0.599 + 0.2028x_t + \eta_t$$
$$\eta_t = 0.6922\eta_{t-1} + \epsilon_t - 0.5758 \epsilon_{t-1} + 0.198 \epsilon_{t-2}$$

Now we will plot both ARIMA and regression errors
```{r}
cbind("Regression Errors" = residuals(fit, type = "regression"),
      "ARIMA Errors" = residuals(fit, type = "innovation")) %>% 
  autoplot(facets = TRUE)
```

It is the ARIMA errors that should resemble a white noise series.
```{r}
checkresiduals(fit)
```

To forecast using a regression model with ARIMA errors, we need to forecast the regression part of the model and the ARIMA part of the model, and combine the results.

In above example we are assuming that future percentage change in income is simply the average of the income in last 40 years.
```{r}
fcast = forecast(fit, xreg = rep(mean(uschange[,"Income"]),8))
autoplot(fcast) + xlab("Year") +
  ylab("Percentage change")
```

The prediction that are calculated do not include the uncertainity associated with the income variable itself. We assume that we know the value of predictors prior forecasting.

**Example 2. Forecasting electricity demand**
```{r}
qplot(Temperature, Demand, data = as.data.frame(elecdaily)) +
  xlab("Temperature") + ylab("Demand")
```

It is clear that the relationship between demand and temperature is non-linear. More electricity is used on cold days due to heating and hot days due to air conditioning.
We have a indicator that mark as workday. It takes value 1 on workday and 0 otherwise.
```{r}
autoplot(elecdaily[,c(1,3)], facets = TRUE, )
```


The plot shows that we need a non-linear and dynamic model. We saw a U-shape in the above graph, so we will a quadratic regression model.

```{r}
xreg = cbind(MaxTemp = elecdaily[,"Temperature"],
      MaxTempsq = elecdaily[,"Temperature"]^2,
      Workday = elecdaily[,"WorkDay"])
fit = auto.arima(elecdaily[,"Demand"], xreg = xreg)
summary(fit)
checkresiduals(fit)
```

The model some significant correlation, Also, we can see some ouliers in the residuals which will effect the prediction intervals.

Using the estimated model we forecast 14 days ahead starting from Thursday 1 January 2015 (a non-work-day being a public holiday for New Years Day). We assume temperature for the next 14 days to a constant 26 degrees.
```{r}
fcast = forecast(fit, xreg = cbind(rep(26,14),
                                   rep(26^2,14),
                                   c(0,1,0,0,1,1,1,1,1,0,0,1,1,1)))
autoplot(fcast) + ylab("Electricity demand (GW)")
```

#### Trends in Dynamic Regression
There are 2 kind of trends. A deterministic trend can be modelled as
$$y_t = \beta_0 + \beta_1t + \eta_t,$$
where $\eta_t$ is an ARMA process. A stochastic process is 
$$y_t = \beta_0 + \beta_1t + \eta_t,$$
where $\eta_t$ is an ARIMA process with d = 1. We can take differences both the sides so that 
$$y_{t}^{'} = \beta_1 + \eta_{t}^{'}$$
where $\eta_{t}^{'}$ is an ARIMA process. In other words,
$$y_t = y_{t-1} + \beta_1 + \eta_{t}^{'}$$

**Example - International visitors to Australia**
```{r}
autoplot(austa) + xlab("Year") +
  ylab("millions of people") +
  ggtitle("Total annual international visitors in Australia")
```

We will first fit deterministic trend to the data.
```{r}
trend = seq_along(austa) 
(fit1 = auto.arima(austa, d = 0, xreg = trend))
```

The model selected is 
$$y_t = 0.4156 + 0.171t + \eta_t$$
$$\eta_t = 1.1127 \eta_{t-1} - 0.3805 \eta_{t-2} + \epsilon_t $$
and $\epsilon_t$ follows NID(0, 0.2979).
The estimated growth in visitor numbers is 0.17 million people per year.

Now, we will fit stochastic trend to the data.
```{r}
(fit2 = auto.arima(austa, d = 1))
```
The model selected is 
$$y_t -y_{t-1} = 0.1735 + \eta_{t}^{'}$$
$$y_t = y_0 + 0.1735t + \eta_t$$
$$\eta_t = \eta_{t-1} + 0.3006 \epsilon_{t-1} + \epsilon_t$$
and $\epsilon_t$ follows NID(0, 0.03376).
In this case, the estimated growth in visitor numbers is also 0.17 million people per year. But the prediction interval from both the models won't be similar.
```{r}
fc1 = forecast(fit1,
               xreg = cbind(trend = length(austa) + 1:10))

fc2 = forecast(fit2, h = 10)
autoplot(austa) +
  autolayer(fc2, "Stochastic trend") +
  autolayer(fc1, "Deterministic trend") +
  ggtitle("Forecasts from trend models") +
  xlab("Year") + ylab("Visitors to Australia (millions)") +
  guides(colour = guide_legend(title = "Forecast"))
```

In deterministic trends the slope of the trend is not going to change over time. On the other hand, stochastic trends can change, and the estimated growth is only assumed to be the average growth over the historical period, not necessarily the rate of growth that will be observed into the future. 

It is safer to forecast with stochastic trends, especially for longer forecast horizons, as the prediction intervals allow for greater uncertainty in future growth.