---
title: "Residual Diagnostics and Forecast Accuracy"
author: "Ankit Tyagi"
date: "12/29/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#########################################################################################################################################################################Residual diagnostic#########################################################
####################################################################################################################

Fitted Values :- 
1. Each observation in a time series can be forecast using all previous observations. These values are called Fitted Values. 
2. Fitted Values are often not the actual forecasts because any parameters involved in the forecasting method are estimated using all the available observation in the time series, including future observations. For e.g. Average Method and Drift Method. But in case of naive and seasonal naive, the forecast do not involve any parameter so the fitted values are actual forecasts.

Residual :- Residual in a time series model is what is left over after fitting a model. 
$$e_t = y_t - fitted(y_t)$$
If a model fit the data accurately then,

1. The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
2.The residual has zero mean. If the residuals have a mean other than zero, then the forecasts are biased. However, adjusting for bias is easy: if the residuals have mean m, then simply add m to all forecasts and the bias problem is solved.

But there is a possobility that more than one forecasting methods satisfy above mentioned properties. So these properties can not be used to select a forecasting method.

Another 2 useful (not necessary) properties to check are 
1. The residuals have a constant variance.
2. The residuals are normally distributed.

The above 2 properties makes the calculation for prediction interval more easy otherwise they do not cause any problem in the forecasting.

Now we will see an example of residual analysis.
```{r}
library(fpp2)
autoplot(goog200) +
  xlab("Day") + ylab("closing prices(in $") +
  ggtitle("Google Stock(daily ending 6 December 2013")
```

For stock prices, naive method is often the best method.

```{r}
autoplot(residuals(naive(goog200))) + 
  xlab("Day") + ylab("") +
  ggtitle("Residual from the Naive Method")
```

The graph above shows that mean of the residuals is close to zero. Also, the variation of the residuals stays same across the historical data, apart from one outlier.
```{r}
gghistogram(residuals(naive(goog200))) + ggtitle("Histogram of the residuals")
```

The histogram shows that the residuals are not normally distributed. There are some extreme observations on the right (positively skewed). The prediction interval calculated assuming the normal distribution may be inaccurate.
```{r}
ggAcf(residuals(naive(goog200))) + ggtitle("ACF of residuals")
```

The autocorrelation is not significant for any lag. 

From above graph it appears that naive method forecasts accounts for all available information.

Portmanteau tests for autocorrelation :-
One issue with ACF plots is that we make an hypothesis for each of the lag and each test has some probability of giving a false result. If the value of h is large, then it is likely that atleast one of them gives wrong result.
To overcome this we are going to test whether the first h autocorrelations are significantly different from what is expected from a white noise process. 

Box - Pierce test :- 
$$ Q = T \sum_{k=1}^{h} r_{k}^{2}\  ,$$
where h = maximum lag being considered.(usually h = 10 for non seasonal data, h = 2m for seasonal data with seasonal frequency m). The test is still not good if h is too large. So if the values are larger than T/5 use h = T/5.

T = no. of observations.

Ljung-Box test :- $$Q = T(T+2)  \sum_{k=1}^{h}(T-k)^{-1} r_{k}^{2}$$
If the autocorrelation comes with white noise then both Q follows chi square distribution with (h-K) degrees of freedom where K is the number of parameters in the model.

```{r}
Box.test(residuals(naive(goog200)), lag = 10, fitdf = 0)
```
```{r}
Box.test(residuals(naive(goog200)), lag = 10, fitdf = 0, type = "Lj")
```

p values are quite high that means that residuals are not distinguishable from a white noise process.

All of the above analysis can be done using just one function.
```{r}
checkresiduals(naive(goog200))
```

Forecast accuracy

The size of the residuals is not a reliable indication of how large true forecast errors are likely to be. The accuracy of forecasts can only be determined by considering how well a model performs on new data that were not used when fitting the model.

We divide the dataset in training data (80% usually) and test set (20% data). 
Forecast error = $Actual(y_{T+h}) - forc(y_{T+h})$.
Forecast error are different from residuals as the residuals are calculated on the training dataset.

Mean absolute error : MAE :- mean(|$e_t$|).
Root squared mean error : RMSE :- $\sqrt{mean(e_t^2)}$.

A method that minimize MAE leads to forecast of the median and a method that minimise RMSE leads to the forecast of mean.

The issue with MAE and RMSE is that the results are on same scale as of the data. So we cannot compare the accurancy of methods that are based on datasets of different units.

Mean absolute percentage error : MAPE :- mean(|$p_t$|).
where $p_t = 100e_t/y_t$.

Disadvantge of using percentage error :- 
1. Percentages are infinite or undefined for $y_t$ = 0 and having extreme values for $y_t$ close to zero.
2. Percentage errors make sense only when the measurement is on interval scale not on the ratio scale because percentage errors assume that the measurement scale has a meaningful zero.
3. Percentage error ususally put high penalty on the negative error than on positive error as there is always more possibility of making a positive error than making a negetive error.

To solve these issues we might use symmetric MAPE(sMAPE)
sMAPE :- mean( 200 * $|y_t - \hat{y}_t|/(y_t + \hat{y}_t)$ )
This still do not solve the issue mentioned in (1) above.

Scaled Error :- To compare the forecast from different time series, we can use scaled error. For the non-seasonal data scaled error can be given as :-
$$q_j = \frac{e_j}{\frac{1}{T-1} \sum_{t=2}^{T} |y_t - \hat{y}_t|}$$
The denominator is measuring the error from a naive forecast method. 
1. $q_j$ is independent of units. 
2. $q_j$ is less than 1 if the forecast is better than the average naive forecast and greater than 1 is the forecast is worse than average naive forecast.

For the seasonal data scaled error can be given as :-
$$q_j = \frac{e_j}{\frac{1}{T-m} \sum_{t=m+1}^{T} |y_t - \hat{y}_{t-m}|}$$
Mean absolute scaled error (MASE) = mean(|$q_t$|).

We will see an example of this on Quarterly Beer Production data. 
```{r}
beer2 = window(ausbeer, start = 1992, end = c(2007,4))
beerfit1 = meanf(beer2, h = 10)
beerfit2 = rwf(beer2, h = 10)
beerfit3 = snaive(beer2, h = 10)
autoplot(window(ausbeer, start = 1992)) +
  autolayer(beerfit1, series = "Mean", PI = FALSE) +
  autolayer(beerfit2, series = "Naive", PI = FALSE) +
  autolayer(beerfit3, series = "Seasonal Naive", PI = FALSE) +
  ggtitle("Quarterly beer production data") +
  xlab("Years") + ylab("Megalitres") +
  guides(colour = guide_legend(title = "Forecast"))
```

The above graph shows that the seasonal naive fits the data more accurately. We will prove the same using error measures.

```{r}
beer3 = window(ausbeer, start = 2008)
accuracy(beerfit1, beer3)
accuracy(beerfit2, beer3)
accuracy(beerfit3, beer3)
```

The seasonal naive method shows the least test error in all the error measures.
Some times different erroe measures might leads to different conclusion about which is the best forecasting method.

Now we will see a non seasonal data example. Google stock price index
```{r}
googfc1 = meanf(goog200, h = 40)
googfc2 = rwf(goog200, h = 40)
googfc3 = rwf(goog200, drift = TRUE, h = 40)
autoplot(subset(goog, end = 240)) +
  autolayer(googfc1, series = "Mean", PI = FALSE) +
  autolayer(googfc2, series = "Naive", PI = FALSE) +
  autolayer(googfc3, series = "Drift", PI = FALSE) +
  ggtitle("Google Daily Stock Price Index") +
  xlab("Day") + ylab("Closing Price(in $)") +
  guides(colour = guide_legend(title = "Forecast"))
```

The above graph shows that the drift method is giving the best results.
```{r}
googtest = window(goog, start = 201, end = 240)
accuracy(googfc1, googtest)
accuracy(googfc2, googtest)
accuracy(googfc3, googtest)
```

All the error measures shows that drift method gives the best forecast.

Time Series Cross Validation :- In this method we do a series of tests each test contains one test observations and the preceeding that observation is the training set. This way we are not using any future observation for forecast.
Initial observations are not used for cross validation as the forecast on a small dataset is not reliable.

```{r}
e = tsCV(goog200, rwf, drift = TRUE, h = 1)
sqrt(mean(e^2, na.rm = TRUE))
```

6.233245 is our average cross validation error.
```{r}
sqrt(mean(residuals(rwf(goog200, drift = TRUE))^2, na.rm  = TRUE))
```

RMSE from the residuals is obviously smaller as we are using all the available observations including future observations to forecast.

We can also do a multistep forecast cross validation.
```{r}
e = tsCV(goog200, forecastfunction = naive, h = 8)

mse = colMeans(e^2, na.rm = TRUE)

data.frame(h = 1:8, MSE = mse) %>% 
  ggplot(aes(x = h, y = MSE)) + geom_point()
```

The above shows how the forecast error is increasing as the forecast horizon is increasing.


Excercise :-

5). Australian Beer Data 
The data is seasonal in nature. So, we are using seasonal naive forecast.
```{r}
beer <- window(ausbeer, start=1992)
fc <- snaive(beer)
autoplot(fc)
```

```{r}
res <- residuals(fc)
autoplot(res)
```

```{r}
checkresiduals(fc)
```

The residuals looks negatively skewed. The last graph shows that they are not normally distributed.

p-value is very small concludes that residuals are autocorrelated.ACF plot shows a spike at lag 4.

Residuals are scattered around zero but some spikes can be seen on the negetive side. Overall it looks like the model does good but can be improved.

6). Internet Usage per minute.
```{r}
autoplot(WWWusage) +
  ggtitle("Internet Usage per Minute") +
  xlab("Minutes") + ylab("No. of Users")
```

The data shows no seasonality. We are going to use naive method to fit the data.
```{r}
fc = naive(WWWusage)
res = residuals(fc)
autoplot(res)
```

```{r}
checkresiduals(fc)
```

Residual plot shows that residuals are not random in nature. There is a possible correlation between the residuals.
ACF also indicates the same. Overall the naive is not fitting the data properly.

Quarterly clay brick production :-
```{r}
autoplot(bricksq) +
  ggtitle("Quarterly clay brick production") +
  xlab("Quarter")
```

The data shows some seasonal patterns so we are going to use seasonal naive method.
```{r}
fc = snaive(bricksq)
res = residuals(fc)
autoplot(res)
```

```{r}
checkresiduals(fc)
```

Residual plot shows that residuals have increasing variance. Also we are highly overestimating in some cases that results in spikes in the residuals on the negetive side.

ACF plots also shows correlation among the residuals.

Seasonal naive method is not fitting the data properly.

Retail Data :-
```{r}
retaildata = readxl::read_excel("/Users/atyagi/Desktop/Time Series Forecasting/Time-Series-Forecasting/Time_series_data/retail.xlsx",skip =1)
myts = ts(retaildata[,"A3349873A"], start = c(1984,4), frequency = 12)
```

Splitting the datasets in train and test.
```{r}
myts.train = window(myts, end = c(2010,12))
myts.test = window(myts, start = 2011)
```

```{r}
autoplot(myts) +
  autolayer(myts.train, series = "Training") +
  autolayer(myts.test, series = "Test")
```

The dataset is seasonal in nature, so we will apply sesasonal naive method.
```{r}
fc = snaive(myts.train)
accuracy(fc, myts.test)
```

```{r}
checkresiduals(fc)
```

Residual plots shows that mean of the residuals is greater than zero. Also the variance of the residuals is increasinf with time.
Residuals are appears to be normally distributed but with a higher peak value.
Residuals are also correlated as can be seen in the ACF plot and Q-test.

The accuracy measures are also giving some contradicting results. RMSE and MASE had increased in the test set but on the other hand MAPE has decreased. This is happening because we have more residuals with positive values and as it has been mentioned earlier that percentage error put more penalty on the positive errors, it can give us a misleading results if the test data is not as identical as the train data.

9). Quarterly visitor nights for various regions of Australia:-
Total quarterly visitor nights (in millions) from 1998-2016 for twenty regions of Australia within six states. We are going to pick just one region QLDMetro.
```{r}
autoplot(visnights[,"QLDMetro"]) 
```

```{r}
train1= window(visnights[,"QLDMetro"], end = c(2015,4))
train2= window(visnights[,"QLDMetro"], end = c(2014,4))
train3= window(visnights[,"QLDMetro"], end = c(2013,4))
fc1 = snaive(train1)
accuracy(fc1,window(visnights[,"QLDMetro"], start = 2016, end = c(2016,4)))
fc2 = snaive(train2)
accuracy(fc2,window(visnights[,"QLDMetro"], start = 2015, end = c(2015,4)))
fc3 = snaive(train3)
accuracy(fc3,window(visnights[,"QLDMetro"], start = 2014, end = c(2014,4)))
```

IBM Stock Prices :-
```{r}
autoplot(ibmclose) +
  ggtitle("IBM Stock Prices") +
  ylab("Stock Price")
  
```

The data has a downward trend and also a cyclic pattern
```{r}
train_ibm = window(ibmclose, end = 300)
test_ibm = window(ibmclose, start = 301)
fc1 = meanf(train_ibm)
fc2 = naive(train_ibm)
fc3 = rwf(train_ibm, drift = TRUE)
accuracy(fc1, test_ibm)
accuracy(fc2, test_ibm)
accuracy(fc3, test_ibm)
```

Naive method gives the best test accuracy among all the methods. Drift is performing a little worse because in test set the time series start increasing again which is probably because of a cyclic pattern that can be seen the time plot above.

```{r}
checkresiduals(fc2)
```
Residuals looks fine at the start but start increasing in size as the time increase.
Ljung test also proves that the residuals are correlated to each other which can also be seen in ACF plot.

Housing Sales Data:-
```{r}
autoplot(hsales) +
  ggtitle("Sales of one-family houses") 
```

The data looks seasonal with a cyclic pattern also present.
```{r}
train_hsales = window(hsales, end = c(1993,12))
test_hsales = window(hsales, start = 1994)
fc1 = meanf(train_hsales)
fc2 = naive(train_hsales)
fc3 = snaive(train_hsales)
accuracy(fc1, test_hsales)
accuracy(fc2, test_hsales)
accuracy(fc3, test_hsales)
```

As expected, seasonal naive method gives the best results as it takes care of the seasonality present in the data.

```{r}
checkresiduals(fc3)
```

We can see that the residuals showing a cyclic pattern that is because the cyclic pattern still exist in the time series.
Residuals are bimodel and negatively skewed.
Residuals are also correlated as can be seen the ACF plot.
