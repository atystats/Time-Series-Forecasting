---
title: "5(c). State Space Models"
author: "Ankit"
date: "2/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### State Space Model :-
A statistical model is a stochastic (or random) data generating process that can produce an entire forecast distribution.

Exponential smoothing methods that we have seen till now can produce only the point foreacasts but the state space model can produce the same forecasts with forecasts distribution as well.

Each model consists of a measurement equation that describes the observed data, and some state equations that describe how the unobserved components or states (level, trend, seasonal) change over time. Hence, these are referred to as state space models.

A state space model is represented as ETS(Error, Trend, Seasonal). 
The possibility for each component are :- Error= {A, M}, Trend= {N, A, $A_d$}, Seasonal= {N, A, M}.

##### ETS(A,N,N) :- Simple Exponential Smoothing with additive error.

The component form of the simple exponential smoothing is 

**Forecast Equation :- ** $\hat y_{t+1|t} = l_t$.

**Smoothing equation :- ** $l_t = αy_t + (1-α)l_{t-1}$.

The above equation can be written as 
$$=> l_{t-1} + α(y_t - l_{t-1})$$
$$=> l_{t-1} + α(y_t - \hat y_{t|t-1})$$
$$=> l_{t-1} + αe_t$$
where $e_t$ is the residual at time t.
The training data error leads to the adjustment in estimated level throughout the smoothing process.

We can also write $y_t = l_{t-1} + e_t$ so that each observation can be presented as previous level plus error. By defining the distribution of errors we can make this into innovation state space model. 
For a model with additive errors, we assume the errors to be normally distributed with mean 0 and a constant variance (or a white noise process).

The equation of the model are :-
$$y_t = l_{t-1} + e_t$$ This is called the **measurement (or observation) equation**.
The measurement equation shows the relationship between the observation and the unobserved states.
In this case, observation $y_t$ is a linear function of the level $l_{t−1}$, the predictable part of $y_t$, and the error $e_t$, the unpredictable part of $y_t$. For other innovations state space models, this relationship may be nonlinear.

$$l_t = l_{t-1} + αe_t$$ This is called the **state (or transition) equation**.
The state equation shows the evolution of the state through time.

These 2 equations together with the statistical distribution of errors constitute the innovation state space model underlying simple exponential smoothing.

##### ETS(M,N,N) :- Simple Exponential Smoothing with multiplicative error.

Similarly, we can specify models with the multiplicative errors. In this case, the training errors are relative errors $$e_t = \frac{y_t - \hat y_{t|t-1}}{\hat y_{t|t-1}}.$$

Multiplicative form of the state space model is given by :-
$$y_t = l_{t-1}(1 + e_t)$$
$$l_t = l_{t-1}(1 + αe_t)$$

##### ETS(A,A,N) :- Holt's Linear Method with additive error.

In this case, the training errors are given as $$e_t = y_t - l_{t-1} - b_{t-1}.$$
State space form is given by :-
$$y_t = l_{t-1}+ b_{t-1} + e_t$$
$$l_t = l_{t-1}+ b_{t-1} + αe_t$$
$$b_t = b_{t-1} + βe_t$$
where $β = αβ^*$.

##### ETS(M,A,N) :- Holt's Linear Method with multiplicative error.

In this case, the training errors are given as $$e_t = \frac{y_t - l_{t-1} - b_{t-1}}{l_{t-1} + b_{t-1}}$$
State space form is given by :-
$$y_t = (l_{t-1}+ b_{t-1})(1 + e_t)$$
$$l_t = (l_{t-1}+ b_{t-1})(1 + αe_t)$$
$$b_t = b_{t-1} + β(l_{t-1}+ b_{t-1})e_t$$

**The rest of the state space model are in the SSM Models.png file in the repository.**

#### Estimating ETS Model 
When it comes to estimating parameters for the state space model, we can use an alternate to the minimising sum of squares as maximise the likelihood. The likelihood is the probability of the data arising from the specified model. For additive models, these two approach gives the similar results but not for multiplicative model.

The values that smoothing parameters can take are restricted. Traditionally, the parameters have been constrained to lie between 0 and 1 so that the equations can be interpreted as weighted averages. The damping parameter ϕ is usually constrained further to prevent numerical difficulties in estimating the model. 

Another way to view the parameters is through a consideration of the mathematical properties of the state space models. The parameters are constrained in order to prevent observations in the distant past having a continuing effect on current forecasts. This leads to some admissibility constraints on the parameters, which are usually (but not always) less restrictive than the traditional constraints region

#### Model Selection 
The AIC, AICc and BIC, can be used here to determine which of the ETS models is most appropriate for a given time series.

$$AIC = -2 log(L) + 2k$$
$$AICc = AIC + \frac{k(k+1)}{T-k-1}$$
$$BIC = AIC + k[log(T) - 2]$$

The ETS model that involves addtive errors and multiplicative seasonality can creates numerical difficulties due to division by values potentially close to zero in the state equations.
Also, models with multiplicative errors are useful when the data are strictly positive, but are not numerically stable when the data contain zeros or negative values.

```{r}
library(fpp2)
aust = window(austourists, start = 2005)
fit = ets(aust)
summary(fit)
```
The model selected is ETS(M,A,M):
$$y_t = (l_{t-1} + b_{t-1})s_{t-m}(1+e_t)$$
$$y_t = (l_{t-1} + b_{t-1})(1+αe_t)$$
$$b_t = b_{t-1} + β(l_{t-1}+ b_{t-1})e_t$$
$$s_t = s_{t-m}(1+γe_t)$$
```{r}
autoplot(fit)
```

The small values of β and γ mean that the slope and seasonal components change very little over time. The narrow prediction intervals indicate that the series is relatively easy to forecast due to the strong trend and seasonality.

In a multiplicative error model, the residuals are not equivalent to the one-step training error.
```{r}
cbind('Residuals' = residuals(fit),
      'Forecast errors' = residuals(fit, type = "response")) %>%
  autoplot(facets = TRUE) + xlab("Year") + ylab("")
```

#### Forecasting with ETS Model
Point forecasts from the models by iterating the equations for t = T+1, T+2,... T+h and setting $e_t$ = 0 for t>T.

$$y_{T+1} = (l_T + b_T) (1+e_{T+1})$$ This means that
$$\hat y_{T+1} = l_T + b_T$$

Similarly,
$$y_{T+2} = (l_{T+1} + b_{T+1}) (1+e_{T+1})$$ This gives
$$\hat y_{T+2} = l_T + 2b_T$$

The forecasts from state space model and Holt's methods are same given that same parameters and model structure is selected.

ETS point forecasts are equal to the medians of the forecast distributions. For models with only additive components, the forecast distributions are normal, so the medians and means are equal. For ETS models with multiplicative errors, or with multiplicative seasonality, the point forecasts will not be equal to the means of the forecast distributions.

```{r}
fit %>% forecast(h = 8) %>%
  autoplot() +
  ylab("International visitor in Australia (millions)")
```

As mentioned earlier that a big advantage of state space model is that we can produce prediction interval. For most ETS model, the prediction interval can be written as
$$\hat y_{T+h|T} \pm cσ_h$$

where c is based on the coverage probability and $σ_h^2$ is the forecast variance value of which is in the "Forecast variance ETS".

For a few ETS models, there are no known formulas for prediction intervals. In these cases, the forecast() function uses simulated future sample paths and computes prediction intervals from the percentiles of these simulated future paths.
